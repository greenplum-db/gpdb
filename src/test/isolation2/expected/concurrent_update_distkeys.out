-- IF we enable the GDD, then the lock maybe downgrade to
-- RowExclusiveLock, when we UPDATE the distribution keys,
-- A SplitUpdate node will add to the Plan, then an UPDATE
-- operator may split to DELETE and INSERT.
-- IF we UPDATE the distribution keys concurrently, the
-- DELETE operator will not execute EvalPlanQual and the
-- INSERT operator can not be *blocked*, so it will
-- generate more tuples in the tables.
-- We raise an error when the GDD is enabled and the
-- distribution keys is updated.

-- create heap table
0: show gp_enable_global_deadlock_detector;
 gp_enable_global_deadlock_detector 
------------------------------------
 off                                
(1 row)
0: create table tab_update_hashcol (c1 int, c2 int) distributed by(c1);
CREATE
0: insert into tab_update_hashcol values(1,1);
INSERT 1
0: select * from tab_update_hashcol;
 c1 | c2 
----+----
 1  | 1  
(1 row)

-- test for heap table
1: begin;
BEGIN
2: begin;
BEGIN
1: update tab_update_hashcol set c1 = c1 + 1 where c1 = 1;
UPDATE 1
2&: update tab_update_hashcol set c1 = c1 + 1 where c1 = 1;  <waiting ...>
1: end;
END
2<:  <... completed>
UPDATE 0
2: end;
END

0: select * from tab_update_hashcol;
 c1 | c2 
----+----
 2  | 1  
(1 row)
0: drop table tab_update_hashcol;
DROP

-- create AO table
0: create table tab_update_hashcol (c1 int, c2 int) with(appendonly=true) distributed by(c1);
CREATE
0: insert into tab_update_hashcol values(1,1);
INSERT 1
0: select * from tab_update_hashcol;
 c1 | c2 
----+----
 1  | 1  
(1 row)

-- test for AO table
1: begin;
BEGIN
2: begin;
BEGIN
1: update tab_update_hashcol set c1 = c1 + 1 where c1 = 1;
UPDATE 1
2&: update tab_update_hashcol set c1 = c1 + 1 where c1 = 1;  <waiting ...>
1: end;
END
2<:  <... completed>
UPDATE 0
2: end;
END
0: select * from tab_update_hashcol;
 c1 | c2 
----+----
 2  | 1  
(1 row)
0: drop table tab_update_hashcol;
DROP
1q: ... <quitting>
2q: ... <quitting>
0q: ... <quitting>

-- enable gdd
-- start_ignore
! gpconfig -c gp_enable_global_deadlock_detector -v on;

! gpstop -rai;
-- end_ignore

-- create heap table
0: show gp_enable_global_deadlock_detector;
 gp_enable_global_deadlock_detector 
------------------------------------
 on                                 
(1 row)
0: create table tab_update_hashcol (c1 int, c2 int) distributed by(c1);
CREATE
0: insert into tab_update_hashcol values(1,1);
INSERT 1
0: select * from tab_update_hashcol;
 c1 | c2 
----+----
 1  | 1  
(1 row)

-- test for heap table
1: begin;
BEGIN
2: begin;
BEGIN
1: update tab_update_hashcol set c1 = c1 + 1 where c1 = 1;
UPDATE 1
2&: update tab_update_hashcol set c1 = c1 + 1 where c1 = 1;  <waiting ...>
1: end;
END
2<:  <... completed>
ERROR:  tuple to be updated was already moved to another segment due to concurrent update  (seg1 127.0.1.1:7003 pid=74220)
2: end;
END
0: select * from tab_update_hashcol;
 c1 | c2 
----+----
 2  | 1  
(1 row)
0: drop table tab_update_hashcol;
DROP

-- create AO table
0: create table tab_update_hashcol (c1 int, c2 int) with(appendonly=true) distributed by(c1);
CREATE
0: insert into tab_update_hashcol values(1,1);
INSERT 1
0: select * from tab_update_hashcol;
 c1 | c2 
----+----
 1  | 1  
(1 row)

-- test for AO table
1: begin;
BEGIN
2: begin;
BEGIN
1: update tab_update_hashcol set c1 = c1 + 1 where c1 = 1;
UPDATE 1
2&: update tab_update_hashcol set c1 = c1 + 1 where c1 = 1;  <waiting ...>
1: end;
END
2<:  <... completed>
UPDATE 0
2: end;
END
0: select * from tab_update_hashcol;
 c1 | c2 
----+----
 2  | 1  
(1 row)
0: drop table tab_update_hashcol;
DROP
1q: ... <quitting>
2q: ... <quitting>
0q: ... <quitting>

-- split update is to implement updating on hash keys,
-- it deletes the tuple and insert a new tuple in a
-- new segment, so it is not easy for other transaction
-- to follow the update link to fetch the new tuple. The
-- other transaction should raise error for such case.
-- the following case should be tested with GDD enabled.
-- See github issue: https://github.com/greenplum-db/gpdb/issues/8919

create table t_splitupdate_raise_error (a int, b int) distributed by (a);
CREATE
insert into t_splitupdate_raise_error values (1, 1);
INSERT 1

-- test delete will throw error
1: begin;
BEGIN
1: update t_splitupdate_raise_error set a = a + 1;
UPDATE 1

2: begin;
BEGIN
2&: delete from t_splitupdate_raise_error;  <waiting ...>

1: end;
END
2<:  <... completed>
ERROR:  tuple to be deleted was already moved to another segment due to concurrent update  (seg1 127.0.1.1:7003 pid=32174)

2: abort;
ABORT
1q: ... <quitting>
2q: ... <quitting>

-- test norm update will throw error
1: begin;
BEGIN
1: update t_splitupdate_raise_error set a = a + 1;
UPDATE 1

2: begin;
BEGIN
2&: update t_splitupdate_raise_error set b = 999;  <waiting ...>

1: end;
END
2<:  <... completed>
ERROR:  tuple to be updated was already moved to another segment due to concurrent update  (seg0 127.0.1.1:7002 pid=74282)

2: abort;
ABORT
1q: ... <quitting>
2q: ... <quitting>

-- select for update might lock tuples on segments,
-- and wait for the XID lock held by splitupdate transaction,
-- it should also raise an error because it is not easy to
-- follow the update link to a new segment.
-- set optimizer off for this case is because orca does not
-- generate lockrows plannode.
1: begin;
BEGIN
1: update t_splitupdate_raise_error set a = a + 1;
UPDATE 1

2: begin;
BEGIN
2: set optimizer = off;
SET
2&: select * from t_splitupdate_raise_error for update;  <waiting ...>

1: end;
END
2<:  <... completed>
ERROR:  tuple to be updated was already moved to another segment due to concurrent update  (seg0 slice1 127.0.1.1:7002 pid=32220)

2: abort;
ABORT
1q: ... <quitting>
2q: ... <quitting>

drop table t_splitupdate_raise_error;
DROP

-- disable gdd
-- start_ignore
! gpconfig -c gp_enable_global_deadlock_detector -v off;

! gpstop -rai;
-- end_ignore
