<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic
  PUBLIC "-//OASIS//DTD DITA Composite//EN" "ditabase.dtd">
<topic id="topic1" xml:lang="en">
  <title id="pz212122">PL/Container Language</title>
  <body>
    <p>This section includes the following information about PL/Container 1.1 and later:</p>
    <ul>
      <li id="pz219023"><xref href="#about_pl_container" type="topic" format="dita"/></li>
      <li id="pz213664" otherprops="pivotal"><xref href="#topic3" type="topic" format="dita"/></li>
      <li><xref href="#upgrade_plcontainer" type="topic" format="dita"/></li>
      <li id="pz213668"><xref href="#topic6" type="topic" format="dita"/>
      </li>
      <li><xref href="#topic_sk1_gdq_dw" format="dita"/></li>
      <li><xref href="#topic_kds_plk_rbb" format="dita"/></li>
    </ul>
  </body>
  <topic id="about_pl_container" xml:lang="en">
    <title id="pz217886">About PL/Container Language Extension</title>
    <body>
      <p>The Greenplum Database PL/Container language extension allows users 
        to create and run Python or R user-defined functions (UDFs) securely, 
        inside a Docker container, on a Greenplum Database system. </p>
      <p>Running the UDFs inside the Docker containers ensures:</p>
      <ul>
        <li>the user code cannot access the OS or the file system of the local host,</li>
        <li>the code cannot introduce any security risks,</li>
        <li>the functions cannot connect back to the Greenplum Database if the container is started with limited or no network access</li>
        <li>the user functions cannot open any unsecure external connections.</li>
      </ul>
      <p>The PL/Container language extension is available as an open source module. For information
        about the module, see the README file in the GitHub repository at <xref
          href="https://github.com/greenplum-db/plcontainer" format="html" scope="external"
          >https://github.com/greenplum-db/plcontainer</xref>.</p>
      
      <section>
        <title>PL/Container Architecture</title>
        
        <fig id="pl_container_image">
          <image placement="break" href="graphics/pl_container_architecture.png" width="650"
            height="550" align="center"/>
        </fig>
        
        <p>Consider a query that selects table data using all available
          segments, and transforms the data using a PL/Container function. In this
          case, the query executor on master, on the first call to a function in a segment container, starts the container on that segment host. 
          It then contacts the running container to obtain the results. The container might respond with an SPI - SQL query executed by the container to
          get some data back from the database - returning the result to the query executor.</p>
        <p>By closing the Greenplum Database session that started the container, the container
          connection is closed, and the container shuts down. .</p>
        <p>A container running in standby mode waits on the socket and does not consume any CPU resources. PL/Container
          memory consumption depends on the amount of data cached in global dictionaries.</p>
        
      </section>
    </body>
  </topic>
  
  <topic id="topic3" xml:lang="en">
    <title id="pz214493">Install PL/Container </title>
    
    <topic id="requirements">
          <title>Prerequisites</title>
      <body>
        <p><b>Warning</b>: PL/Container is not supported when Greenplum Database is run within a
          Docker container.</p>
        <ol>
          <li>For PL/Container 1.x and 2.0.x use Greenplum Database 5.2.x and later on Red Hat
            Enterprise Linux (RHEL) 7.x (or later) and CentOS 7.x (or later). </li>
          <li>For PL/Container 2.1.0 and later use Greenplum Database 6 on CentOS 7.x (or later),
            RHEL 7.x (or later), or Ubuntu 18.04. <note>PL/Container 2.1.0 and later supports Docker
              images with Python 3 installed.</note></li>
          <li>You have minimum Linux OS kernel version 3.10. To chec your kernal release:
            <codeblock>$ uname -r</codeblock>
          </li>
          <li>The minimum Docker versions that must be installed on Greenplum Database hosts
            (master, primary and all standby hosts): <ul id="ul_z2t_bxd_rbb">
              <li>For PL/Container 1.x or 2.0.x on RHEL or CentOS 7.x - Docker 17.05</li>
              <li>
                <p>For PL/Container 2.1.x on CentOS or RHEL 7.x, or Ubuntu 18.04 - Docker 19.03</p>
              </li>
              <li>On each Greenplum Database host, the <codeph>gpadmin</codeph> user should be part
                of the <codeph>docker</codeph> group for the user to be able to manage Docker images
                and containers.</li>
            </ul></li>
        </ol>
      </body>
    </topic>
    
    <topic id="topic_ydt_rtc_rbb">
      <title>Install Docker</title>
      <body>
        <p>A Docker <i>container</i> is a Linux process that runs in a managed
          way by using Linux kernel features such as cgroups, namespaces and union file systems. For
          information about Docker, see the Docker web site <xref href="https://www.docker.com/"
            format="html" scope="external">https://www.docker.com/</xref>. </p>
        <p>To use PL/Container install Docker on all Greenplum Database host systems. The
          these instructions show how to set up the Docker service on CentOS 7 but RHEL 7
          is a similar process.</p>
        <p>These steps install the docker package and start the Docker service as a user with sudo privileges. </p>
        
        <ol>
          <li>The CentOS <codeph>extras</codeph> repository is accessible.</li>
          <li>The user has sudo privileges or is root.</li>
          <li>Install dependencies required for
            Docker<codeblock>sudo yum install -y yum-utils device-mapper-persistent-data lvm2</codeblock></li>
          <li>Add the Docker
            repo<codeblock>sudo yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo</codeblock></li>
          <li>Update yum cache<codeblock>sudo yum makecache fast</codeblock></li>
          <li>Install Docker<codeblock>sudo yum -y install docker-ce</codeblock></li>
          <li>Start Docker daemon.<codeblock>sudo systemctl start docker</codeblock></li>
          <li>Assign the Greenplum Database administrator <codeph>gpadmin</codeph> to the group
            docker, to give access to the Docker daemon and docker commands:
            <codeblock>sudo usermod -aG docker gpadmin</codeblock></li>
          <li>Exit the session and login again to update the privileges.</li>
          <li>Configure Docker to start when the host system
            starts.<codeblock>sudo systemctl enable docker.service</codeblock><codeblock>sudo systemctl start docker.service</codeblock></li>
          <li>Run a Docker command to test the Docker installation. This command lists the currently
            running Docker containers. <codeblock>docker ps</codeblock></li>
          <li>After you install Docker on all Greenplum Database hosts, restart the Greenplum
            Database system to give Greenplum Database access to Docker.
            <codeblock>gpstop -ra</codeblock></li>
          <li>On each Greenplum Database host, the gpadmin user is part of the docker group to
            manage Docker images and containers. </li>
        </ol>
                  
        <p>See also the Docker site installation instructions for CentOS <xref
            href="https://docs.docker.com/engine/installation/linux/centos/" format="html"
            scope="external">https://docs.docker.com/engine/installation/linux/centos/</xref>. For a
          list of Docker commands, see the Docker engine Run Reference <xref
            href="https://docs.docker.com/engine/reference/run/" format="html" scope="external"
            >https://docs.docker.com/engine/reference/run/</xref>.</p> 
        
      </body>
    </topic>
      
    <topic id="topic_ifk_2tr_dw" otherprops="pivotal">
      <title>Install PL/Container </title>
      <!--Pivotal content-->
      <body>
        <p>Install the PL/Container language extension using the <codeph>gppkg</codeph> utility
          (PL/Container 1.1 and later).</p>
        <ol id="ul_w5b_nzp_dw">
          <li>Download the PL/Container package version that applies to your Greenplum Database
            version, from <xref href="https://network.pivotal.io" format="html" scope="external"
              >Pivotal Network</xref>. PL/Container is listed under Greenplum Database Language
            extensions.  </li>
          <li>As <codeph>gpadmin</codeph>, copy the PL/Container language extension package to the
            master host.</li>
          <li>Make sure Greenplum Database is up and running:<codeblock>gpstate -s</codeblock>If
            it's not, bring it up with this command.<codeblock>gpstart -a</codeblock></li>
          <li>Run the package installation
            command.<codeblock>gppkg -i plcontainer-2.1.0-rhel7-x86_64.gppkg</codeblock></li>
          <li>Source the file
            <codeph>$GPHOME/greenplum_path.sh</codeph>.<codeblock>source $GPHOME/greenplum_path.sh</codeblock></li>
          <li>Restart Greenplum Database.<codeblock>gpstop -ra</codeblock></li>
          <li>Login into one of the available databases, for example, postgres:
            <codeblock>psql postgres;</codeblock></li>
          <li>Register the PL/Container extension, which installs the <codeph>plcontainer</codeph>
            utility and creates PL/Container-specific functions and
            views:<codeblock>CREATE EXTENSION plcontainer; </codeblock>For PL/Container 1.0, the
            command
            is:<codeblock>psql -d <varname>your_database</varname> -f $GPHOME/share/postgresql/plcontainer/plcontainer_install.sql</codeblock></li>
        </ol>
        <p>Now, using the <codeph>plcontainer</codeph> utility you can manage Docker images and the
          PL/Container configuration. </p>
      </body>
    </topic>
      
    <topic id="topic_tcm_htd_gw">
      <title>PL/Container Docker Images</title>
      <body>
        <p>A Docker <i>image</i> is the basis of a container. A Docker container is a running instance
          of a Docker image. When you start a Docker container you specify a Docker image. A Docker
          image is the collection of root filesystem changes and execution parameters that are used
          when you run a Docker container on the host system. An image does not have state and never
          changes. </p>
        <p>PL/Python and PL/R image are available from the Greenplum Database product download site of
          Pivotal Network at <xref href="https://network.pivotal.io/products/pivotal-gpdb"
            format="html" scope="external">https://network.pivotal.io/</xref>.<ul id="ul_epg_t2v_qbb">
              <li>PL/Container for Python 3 - Docker image with Python 3.7 and the Python Data Science
                Module package installed. <note>PL/Container 2.1.0 and later supports Docker images with
                  Python 3 installed.</note></li>
              <li>PL/Container for Python 2 - Docker image with Python 2.7.12 and the Python Data
                Science Module package installed.</li>
              <li>PL/Container for R - A Docker image with container with R-3.3.3 and the R Data Science
                Library package installed. </li>
            </ul></p>
        <p>The Data Science packages contain a set of Python modules or R functions and data sets
          related to data science. <ph otherprops="pivotal ">For information about the packages, see
            <xref href="../install_guide/install_python_dsmod.xml" format="dita">Python Data Science
              Module Package</xref> and <xref href="../install_guide/install_r_dslib.xml"
                format="dita">R Data Science Library Package</xref>.</ph></p>
        <p>The Docker image tag represents the PL/Container release version (for example, 2.1.0). For
          example, the full Docker image name for the PL/Container for Python Docker image is similar
          to <codeph>pivotaldata/plc_python_shared:2.1.0</codeph>. This is the name that is referred
          to in the default PL/Container configuration. Also, you can create custom Docker images,
          install the image and add the image to the PL/Container configuration. </p>
      </body>
    </topic>
    
    <topic id="topic_qcr_bfk_rbb">
      <title>Installing PL/Container Docker Images</title>
      <body>
        <p>The PL/Container language extension includes the <codeph>plcontainer</codeph> utility that
          installs Docker images on the Greenplum Database hosts and adds configuration information to
          the PL/Container configuration file. The configuration information allows PL/Container to
          create Docker containers with the Docker images. For information about
          <codeph>plcontainer</codeph>, see <xref href="#topic_rw3_52s_dw" format="dita"/>.</p>
        <!--Pivotal conent-->
        <p otherprops="pivotal">Download the <codeph>tar.gz</codeph> file that contains the Docker
          images from <xref href="https://network.pivotal.io/products/pivotal-gpdb" scope="external"
            format="html" class="- topic/xref ">Pivotal Network</xref>. <ul id="ul_vsj_pxb_tbb">
              <li><codeph>plcontainer-python3-image-&lt;version>.tar.gz</codeph></li>
              <li><codeph>plcontainer-python-image-&lt;version>.tar.gz</codeph></li>
              <li><codeph>plcontainer-r-image-&lt;version>.tar.gz</codeph></li>
            </ul></p>
        <!--oss only conent-->
        <p otherprops="oss-only">The PL/Container open source module contains dockerfiles to build
          Docker images that can be used with PL/Container. You can build a Docker image to run
          PL/Python UDFs and a Docker image to run PL/R UDFs. See the dockerfiles in the GitHub
          repository at <xref href="https://github.com/greenplum-db/plcontainer" format="html"
            scope="external">https://github.com/greenplum-db/plcontainer</xref>.</p>
        <p>Install Docker images on all the Greenplum Database hosts with the <codeph>plcontainer
          image-add</codeph> command. These examples assume that the Docker images are in files
          located in <codeph>/home/gpadmin</codeph>.
          <codeblock># Install a Python 2 based Docker image
            plcontainer image-add -f /home/gpadmin/plcontainer-python-image-2.1.0-gp6.tar.gz
            
            # Install a Python 3 based Docker image
            plcontainer image-add -f /home/gpadmin/plcontainer-python3-image-2.1.0-gp6.tar.gz
            
            # Install an R based Docker image
            plcontainer image-add -f /home/gpadmin/plcontainer-r-image-2.1.0-gp6.tar.gz</codeblock></p>
        <p>The utility displays progress information as it installs the Docker image on the Greenplum
          Database hosts. </p>
        <p>Use the <codeph>plcontainer image-list</codeph> command to display the installed Docker
          images on the local host.</p>
        <p>Use the <codeph>plcontainer runtime-add</codeph> command to add information to the
          PL/Container configuration file so that PL/Container can access the Docker image. The
          <codeph>-r</codeph> option specifies the runtime ID that you choose. The
          <codeph>-l</codeph> option specifies the language that is contained in the Docker image.
          <codeblock># Add a Python 2 based runtime
            plcontainer runtime-add -r plc_py -i pivotaldata/plcontainer_python_shared:devel -l python
            
            # Add a Python 3 based runtime that is supported with PL/Container 2.1.0 and later
            plcontainer runtime-add -r plc_py3 -i pivotaldata/plcontainer_python3_shared:devel -l python3
            
            # Add an R based runtime
            plcontainer runtime-add -r plc_r -i pivotaldata/plcontainer_r_shared:devel -l r</codeblock></p>
        <p>The utility displays progress information as it updates the PL/Container configuration file
          on the Greenplum Database instances.</p>
        <p>You can view the PL/Container configuration information with the <codeph>plcontainer
          runtime-show -r &lt;runtime_id></codeph> command. You can view the PL/Container
          configuration XML file with the <codeph>plcontainer runtime-edit</codeph> command. </p>
      </body>
    </topic> 
    
 
  <topic id="topic_vf5_lqm_vkb">
    <title>Configure PL/Container</title>
    <body>
      <p>The Greenplum Database utility <codeph>plcontainer</codeph> manages the PL/Container
        configuration files in a Greenplum Database system. The utility ensures that the
        configuration files are consistent across the Greenplum Database master and segment
        instances.</p>
      <note type="warning"> Modifying the configuration files on the segment instances without using
        the utility might create different, incompatible configurations on different Greenplum
        Database segments that could cause unexpected behavior. </note>
      <p>Configuration changes that are made with the utility are applied to the XML files on all
        Greenplum Database segments. However, PL/Container configurations of currently running
        sessions use the configuration that existed during session start up. To update the
        PL/Container configuration in a running session, execute this command in the session.</p>
      <codeblock>SELECT * FROM plcontainer_refresh_config;</codeblock>
      <p>Running the command executes a PL/Container function that updates the session configuration
        on the master and segment instances.</p>
    </body>
    <topic id="topic_wf5_lqm_vkb">
      <title>PL/Container Configuration File</title>
      <body>
        <p>PL/Container maintains a configuration file
            <codeph>plcontainer_configuration.xml</codeph> in the data directory of all Greenplum
          Database segments. The PL/Container configuration file is an XML file. In the XML file,
          the root element <codeph>configuration</codeph> contains one or more
            <codeph>runtime</codeph> elements. You specify the <codeph>id</codeph> of the
            <codeph>runtime</codeph> element in the <codeph># container:</codeph> line of a
          PL/Container function definition. </p>
        <p>In an XML file, names, such as element and attribute names, and values are case
          sensitive.</p>
        <p>This is an example
          file.<codeblock>&lt;?xml version="1.0" ?>
&lt;configuration>
    &lt;runtime>
        &lt;id>plc_python_example1&lt;/id>
        &lt;image>pivotaldata/plcontainer_python_with_clients:0.1&lt;/image>
        &lt;command>./pyclient&lt;/command>
    &lt;/runtime>
    &lt;runtime>
        &lt;id>plc_python_example2&lt;/id>
        &lt;image>pivotaldata/plcontainer_python_without_clients:0.1&lt;/image>
        &lt;command>/clientdir/pyclient.sh&lt;/command>
        &lt;shared_directory access="ro" container="/clientdir" host="/usr/local/greenplum-db/bin/plcontainer_clients"/>
        &lt;setting memory_mb="512"/>
        &lt;setting use_container_logging="yes"/>
        &lt;setting cpu_share="1024"/>
        &lt;setting resource_group_id="16391"/>
    &lt;/runtime>
    &lt;runtime>
        &lt;id>plc_r_example&lt;/id>
        &lt;image>pivotaldata/plcontainer_r_without_clients:0.2&lt;/image>
        &lt;command>/clientdir/rclient.sh&lt;/command>
        &lt;shared_directory access="ro" container="/clientdir" host="/usr/local/greenplum-db/bin/plcontainer_clients"/>
        &lt;setting use_container_logging="yes"/>
        &lt;setting roles="gpadmin,user1"/>
    &lt;/runtime>
    &lt;runtime>
&lt;/configuration></codeblock></p>
        <p>These are the XML elements and attributes in a PL/Container configuration file.</p>
        <parml>
          <plentry>
            <pt>configuration</pt>
            <pd>Root element for the XML file.</pd>
          </plentry>
          <plentry>
            <pt>runtime</pt>
            <pd>One element for each specific container available in the system. These are child
              elements of the <codeph>configuration</codeph> element.</pd>
            <pd>
              <parml>
                <plentry>
                  <pt>id</pt>
                  <pd>Required. The value is used to reference a Docker container from a
                    PL/Container user-defined function. The <codeph>id</codeph> value must be unique
                    in the configuration. The <codeph>id</codeph> must start with a character or
                    digit (a-z, A-Z, or 0-9) and can contain characters, digits, or the characters
                      <codeph>_</codeph> (underscore), <codeph>.</codeph> (period), or
                      <codeph>-</codeph> (dash). Maximum length is 63 Bytes.<p>The
                        <codeph>id</codeph> specifies which Docker image to use when PL/Container
                      creates a Docker container to execute a user-defined function.</p></pd>
                </plentry>
                <plentry>
                  <pt>image</pt>
                  <pd>
                    <p>Required. The value is the full Docker image name, including image tag. The
                      same way you specify them for starting this container in Docker. Configuration
                      allows to have many container objects referencing the same image name, this
                      way in Docker they would be represented by identical containers. </p>
                    <p>For example, you might have two <codeph>runtime</codeph> elements, with
                      different <codeph>id</codeph> elements, <codeph>plc_python_128</codeph> and
                        <codeph>plc_python_256</codeph>, both referencing the Docker image
                        <codeph>pivotaldata/plcontainer_python:1.0.0</codeph>. The first
                        <codeph>runtime</codeph> specifies a 128MB RAM limit and the second one
                      specifies a 256MB limit that is specified by the <codeph>memory_mb</codeph>
                      attribute of a <codeph>setting</codeph> element.</p>
                  </pd>
                </plentry>
                <plentry>
                  <pt>command</pt>
                  <pd>Required. The value is the command to be run inside of container to start the
                    client process inside in the container. When creating a <codeph>runtime</codeph>
                    element, the <codeph>plcontainer</codeph> utility adds a
                      <codeph>command</codeph> element based on the language (the
                      <codeph>-l</codeph> option).</pd>
                  <pd><codeph>command</codeph> element for the Python 2
                    language.<codeblock>&lt;command>/clientdir/pyclient.sh&lt;/command></codeblock></pd>
                  <pd><codeph>command</codeph> element for the Python 3
                    language.<codeblock>&lt;command>/clientdir/pyclient3.sh&lt;/command></codeblock></pd>
                  <pd><codeph>command</codeph> element for the R
                    language.<codeblock>&lt;command>/clientdir/rclient.sh&lt;/command></codeblock></pd>
                  <pd>You should modify the value only if you build a custom container and want to
                    implement some additional initialization logic before the container starts.
                      <note>This element cannot be set with the <codeph>plcontainer</codeph>
                      utility. You can update the configuration file with the <codeph>plcontainer
                        runtime-edit</codeph> command.</note></pd>
                </plentry>
                <plentry>
                  <pt>shared_directory</pt>
                  <pd>Optional. This element specifies a shared Docker shared volume for a container
                    with access information. Multiple <codeph>shared_directory</codeph> elements are
                    allowed. Each <codeph>shared_directory</codeph> element specifies a single
                    shared volume. XML attributes for the <codeph>shared_directory</codeph>
                      element:<ul id="ul_xf5_lqm_vkb">
                      <li><codeph>host</codeph> - a directory location on the host system.</li>
                      <li><codeph>container</codeph> - a directory location inside of
                        container.</li>
                      <li><codeph>access</codeph> - access level to the host directory, which can be
                        either <codeph>ro</codeph> (read-only) or <codeph>rw</codeph> (read-write).
                      </li>
                    </ul></pd>
                  <pd>When creating a <codeph>runtime</codeph> element, the
                      <codeph>plcontainer</codeph> utility adds a <codeph>shared_directory</codeph>
                    element.<codeblock>&lt;shared_directory access="ro" container="/clientdir" host="/usr/local/greenplum-db/bin/plcontainer_clients"/></codeblock></pd>
                  <pd>For each <codeph>runtime</codeph> element, the <codeph>container</codeph>
                    attribute of the <codeph>shared_directory</codeph> elements must be unique. For
                    example, a <codeph>runtime</codeph> element cannot have two
                      <codeph>shared_directory</codeph> elements with attribute
                      <codeph>container="/clientdir"</codeph>. <note type="warning">Allowing
                      read-write access to a host directory requires special consideration.<ul
                        id="ul_yf5_lqm_vkb">
                        <li>When specifying read-write access to host directory, ensure that the
                          specified host directory has the correct permissions. </li>
                        <li>When running PL/Container user-defined functions, multiple concurrent
                          Docker containers that are running on a host could change data in the host
                          directory. Ensure that the functions support multiple concurrent access to
                          the data in the host directory.</li>
                      </ul></note></pd>
                </plentry>
                <plentry>
                  <pt>settings</pt>
                  <pd>Optional. This element specifies Docker container configuration information.
                    Each <codeph>setting</codeph> element contains one attribute. The element
                    attribute specifies logging, memory, or networking information. For example,
                    this element enables
                    logging.<codeblock>&lt;setting use_container_logging="yes"/></codeblock></pd>
                  <pd>These are the valid attributes.<parml>
                      <plentry>
                        <pt>cpu_share</pt>
                        <pd>Optional. Specify the CPU usage for each PL/Container container in the
                          runtime. The value of the element is a positive integer. The default value
                          is 1024. The value is a relative weighting of CPU usage compared to other
                          containers. </pd>
                        <pd>For example, a container with a <codeph>cpu_share</codeph> of 2048 is
                          allocated double the CPU slice time compared with container with the
                          default value of 1024.</pd>
                      </plentry>
                      <plentry>
                        <pt>memory_mb="<varname>size</varname>"</pt>
                        <pd>Optional. The value specifies the amount of memory, in MB, that each
                          container is allowed to use. Each container starts with this amount of RAM
                          and twice the amount of swap space. The container memory consumption is
                          limited by the host system <codeph>cgroups</codeph> configuration, which
                          means in case of memory overcommit, the container is terminated by the
                          system.</pd>
                      </plentry>
                      <plentry>
                        <pt>resource_group_id="<varname>rg_groupid</varname>"</pt>
                        <pd>Optional. The value specifies the <codeph>groupid</codeph> of the
                          resource group to assign to the PL/Container runtime. The resource group
                          limits the total CPU and memory resource usage for all running containers
                          that share this runtime configuration. You must specify the
                            <codeph>groupid</codeph> of the resource group. If you do not assign a
                          resource group to a PL/Container runtime configuration, its container
                          instances are limited only by system resources. For information about
                          managing PL/Container resources, see <xref href="#topic_resmgmt"
                            format="dita">About PL/Container Resource Management</xref>.</pd>
                      </plentry>
                      <plentry>
                        <pt>roles="<varname>list_of_roles</varname>"</pt>
                        <pd>Optional. The value is a Greenplum Database role name or a
                          comma-separated list of roles. PL/Container runs a container that uses the
                          PL/Container runtime configuration only for the listed roles. If the
                          attribute is not specified, any Greenplum Database role can run an
                          instance of this container runtime configuration. For example, you create
                          a UDF that specifies the <codeph>plcontainer</codeph> language and
                          identifies a <codeph># container:</codeph> runtime configuration that has
                          the <codeph>roles</codeph> attribute set. When a role (user) runs the UDF,
                          PL/Container checks the list of roles and runs the container only if the
                          role is on the list.</pd>
                      </plentry>
                      <plentry>
                        <pt> use_container_logging="{yes | no}"</pt>
                        <pd>Optional. Enables or disables Docker logging for the container. The
                          attribute value <codeph>yes</codeph> enables logging. The attribute value
                            <codeph>no</codeph> disables logging (the default). </pd>
                        <pd>The Greenplum Database server configuration parameter <codeph><xref
                              href="../ref_guide/config_params/guc-list.xml#log_min_messages"
                              scope="peer">log_min_messages</xref></codeph> controls the
                          PL/Container log level. The default log level is <codeph>warning</codeph>.
                          For information about PL/Container log information, see <xref
                            href="#plc_notes" format="dita">Notes</xref>.</pd>
                        <pd>
                          <p>By default, the PL/Container log information is sent to a system
                            service. On Red Hat 7 or CentOS 7 systems, the log information is sent
                            to the <codeph>journald</codeph> service. On Red Hat 6 or CentOS 6
                            systems, the log is sent to the <codeph>syslogd</codeph> service. </p>
                        </pd>
                      </plentry>
                    
                    </parml></pd>
                </plentry>
              </parml>
            </pd>
          </plentry>
        </parml>
      </body>
    </topic>
    
    
    <topic id="topic_zf5_lqm_vkb">
      <title>Updating the PL/Container Configuration</title>
      <body>
        <p>You can add a <codeph>runtime</codeph> element to the PL/Container configuration file
          with the <codeph>plcontainer runtime-add</codeph> command. The command options specify
          information such as the runtime ID, Docker image, and language. You can use the
            <codeph>plcontainer runtime-replace</codeph> command to update an existing
            <codeph>runtime</codeph> element. The utility updates the configuration file on the
          master and all segment instances.</p>
        <p>The PL/Container configuration file can contain multiple <codeph>runtime</codeph>
          elements that reference the same Docker image specified by the XML element
            <codeph>image</codeph>. In the example configuration file, the <codeph>runtime</codeph>
          elements contain <codeph>id</codeph> elements named <codeph>plc_python_128</codeph> and
            <codeph>plc_python_256</codeph>, both referencing the Docker container
            <codeph>pivotaldata/plcontainer_python:1.0.0</codeph>. The first
            <codeph>runtime</codeph> element is defined with a 128MB RAM limit and the second one
          with a 256MB RAM limit.</p>
        <codeblock>&lt;configuration>
  &lt;runtime>
    &lt;id>plc_python_128&lt;/id>
    &lt;image>pivotaldata/plcontainer_python:1.0.0&lt;/image>
    &lt;command>./client&lt;/command>
    &lt;shared_directory access="ro" container="/clientdir" host="/usr/local/gpdb/bin/plcontainer_clients"/>
    &lt;setting memory_mb="128"/>
  &lt;/runtime>
  &lt;runtime>
    &lt;id>plc_python_256&lt;/id>
    &lt;image>pivotaldata/plcontainer_python:1.0.0&lt;/image>
    &lt;command>./client&lt;/command>
    &lt;shared_directory access="ro" container="/clientdir" host="/usr/local/gpdb/bin/plcontainer_clients"/>
    &lt;setting memory_mb="256"/>
    &lt;setting resource_group_id="16391"/>
  &lt;/runtime>
&lt;configuration></codeblock>
      </body>
    </topic>

    
    
    <topic id="topic_ag5_lqm_vkb">
      <title>Notes</title>
      <body>
        <ul id="ul_bg5_lqm_vkb">
          <li>PL/Container does not support the Greenplum Database domain object.</li>
          <li>PL/Container maintains the configuration file
              <codeph>plcontainer_configuration.xml</codeph> in the data directory of all Greenplum
            Database segment instances: master, standby master, primary, and mirror. This query
            lists the Greenplum Database system data
              directories:<codeblock>SELECT hostname, datadir FROM gp_segment_configuration;</codeblock><p>A
              sample PL/Container configuration file is in
                <codeph>$GPHOME/share/postgresql/plcontainer</codeph>. </p></li>
          <li>When Greenplum Database executes a PL/Container UDF, Query Executer (QE) processes
            start Docker containers and reuse them as needed. After a certain amount of idle time, a
            QE process quits and destroys its Docker containers. You can control the amount of idle
            time with the Greenplum Database server configuration parameter <codeph><xref
                href="../ref_guide/config_params/guc-list.xml#gp_vmem_idle_resource_timeout"
                scope="peer">gp_vmem_idle_resource_timeout</xref></codeph>. Controlling the idle
            time might help with Docker container reuse and avoid the overhead of creating and
            starting a Docker container. <note type="warning">Changing
                <codeph>gp_vmem_idle_resource_timeout</codeph> value, might affect performance due
              to resource issues. The parameter also controls the freeing of Greenplum Database
              resources other than Docker containers.</note></li>
          <li>If a PL/Container Docker container exceeds the maximum allowed memory, it is
            terminated and an out of memory warning is displayed. On Red Hat 6 or CentOS 6 systems
            that are configured with Docker version 1.7.1, the out of memory warning is also
            displayed if the PL/Container Docker container main program (PID 1) is terminated.</li>
          <li>In some cases, when PL/Container is running in a high concurrency environment, the
            Docker daemon hangs with log entries that indicate a memory shortage. This can happen
            even when the system seems to have adequate free memory.<p>The issue seems to be
              triggered by a combination of two factors, the aggressive virtual memory requirement
              of the Go language (<codeph>golang</codeph>) runtime that is used by PL/Container, and
              the Greenplum Database Linux server kernel parameter setting for
                <codeph>overcommit_memory</codeph>. The parameter is set to 2 which does not allow
              memory overcommit. </p><p>A workaround that might help is to increase the amount of
              swap space and increase the Linux server kernel parameter
                <codeph>overcommit_ratio</codeph>. If the issue still occurs after the changes,
              there might be memory shortage. You should check free memory on the system and add
              more RAM if needed. You can also decrease the cluster load.</p></li>
          <li>PL/Container does not limit the Docker base device size, the size of the Docker
            container. In some cases, the Docker daemon controls the base device size. For example,
            if the Docker storage driver is devicemapper, the Docker daemon
              <codeph>--storage-opt</codeph> option flag <codeph>dm.basesize</codeph> controls the
            base device size. The default base device size for devicemapper is 10GB. The Docker
            command <codeph>docker info</codeph> displays Docker system information including the
            storage driver. The base device size is displayed in Docker 1.12 and later. For
            information about Docker storage drivers, see the Docker information <xref
              href="https://docs.docker.com/engine/reference/commandline/dockerd/#daemon-storage-driver"
              format="html" scope="external">Daemon storage-driver</xref>. <p>When setting the
              Docker base device size, the size must be set on all Greenplum Database
            hosts.</p></li>
          <li>When PL/Container logging is enabled, you can set the log level with the Greenplum
            Database server configuration parameter <codeph><xref
                href="../ref_guide/config_params/guc-list.xml#log_min_messages" scope="peer"
                >log_min_messages</xref></codeph>. The default log level is
            <codeph>warning</codeph>. The parameter controls the PL/Container log level and also
            controls the Greenplum Database log level.<ul id="ul_cg5_lqm_vkb">
              <li>PL/Container logging is enabled or disabled for each runtime ID with the
                  <codeph>setting</codeph> attribute <codeph>use_container_logging</codeph>. The
                default is no logging. </li>
              <li>The PL/Container log information is the information from the UDF that is run in
                the Docker container. By default, the PL/Container log information is sent to a
                system service. On Red Hat 7 or CentOS 7 systems, the log information is sent to the
                  <codeph>journald</codeph> service. On Red Hat 6 or CentOS 6 systems, the log
                information is sent to the <codeph>syslogd</codeph> service. The PL/Container log
                information is sent to the log file of the host were the Docker container runs. </li>
              <li>The Greenplum Database log information is sent to log file on the Greenplum
                Database master.</li>
            </ul><p>When testing or troubleshooting a PL/Container UDF, you can change the Greenplum
              Database log level with the <codeph>SET</codeph> command. You can set the parameter in
              the session before you run your PL/Container UDF. This example sets the log level to
                <codeph>debug1</codeph>.</p><codeblock>SET log_min_messages='debug1' ;</codeblock>
            <note>The parameter <codeph>log_min_messages</codeph> controls both the Greenplum
              Database and PL/Container logging, increasing the log level might affect Greenplum
              Database performance even if a PL/Container UDF is not running.</note></li>
        </ul>
      </body>
    </topic>
  </topic>
  
    <topic id="upgrade_plcontainer">
      <title>Upgrade PL/Container</title>
      <body>
        <p>To upgrade PL/Container, follow the steps described below.</p>
        <section>
          <title>Upgrade from PL/Container 1.0</title>
          <p>To upgrade to PL/Container 1.1 or later, uninstall version 1.0 and install the new
            version. You cannot use the <codeph>gppkg</codeph> option <codeph>-u</codeph>. The
              <codeph>gppkg</codeph> utility installs PL/Container 1.1 and later as a Greenplum
            Database extension, while PL/Container 1.0 is installed as a Greenplum Database
            language. The Docker images and the PL/Container configuration do not change when
            upgrading PL/Container, only the PL/Container extension installation changes.</p>
          <p>As part of the upgrade process, you must drop PL/Container from all databases that are
            configured with PL/Container.</p>
          <note type="important">Dropping PL/Container from a database drops all PL/Container UDFs
            from the database, including user-created PL/Container UDFs. If the UDFs are required,
            ensure you can re-create the UDFs before dropping PL/Container. This
              <codeph>SELECT</codeph> command lists the names of and body of PL/Container UDFs in a
              database.<codeblock>SELECT proname, prosrc FROM pg_proc WHERE prolang = (SELECT oid FROM pg_language WHERE lanname = 'plcontainer');</codeblock><p>For
              information about the catalog tables, <codeph>pg_proc</codeph> and
                <codeph>pg_language</codeph>, see <xref
                href="../ref_guide/system_catalogs/catalog_ref-tables.xml" format="dita"
                scope="peer">System Tables</xref>. </p></note>
          <p>These steps upgrade from PL/Container 1.0 to PL/Container 1.1 or later in a database.
            The steps save the PL/Container 1.0 configuration and restore the configuration for use
            with PL/Container 1.1 or later.<ol id="ol_j1j_bml_vkb">
              <li>Save the PL/Container configuration. This example saves the configuration to
                  <codeph>plcontainer10-backup.xml</codeph> in the local
                directory.<codeblock>plcontainer runtime-backup -f plcontainer10-backup.xml</codeblock></li>
              <li>Remove any <codeph>setting</codeph> elements that contain the
                  <codeph>use_container_network</codeph> attribute from the configuration file. For
                example, this <codeph>setting</codeph> element must be removed from the
                configuration
                file.<codeblock>&lt;setting use_container_network="yes"/></codeblock></li>
              <li>Run the <codeph>plcontainer_uninstall.sql</codeph> script as the
                  <codeph>gpadmin</codeph> user for each database that is configured with
                PL/Container. For example, this command drops the <codeph>plcontainer</codeph>
                language in the <codeph>mytest</codeph> database.
                  <codeblock>psql -d mytest -f $GPHOME/share/postgresql/plcontainer/plcontainer_uninstall.sql</codeblock><p>The
                  script drops the <codeph>plcontainer</codeph> language with the
                    <codeph>CASCADE</codeph> clause that drops PL/Container-specific functions and
                  views in the database. </p></li>
              <li>Use the Greenplum Database <codeph>gppkg</codeph> utility with the
                  <codeph>-r</codeph> option to uninstall the PL/Container language extension. This
                example uninstalls the PL/Container language extension on a Linux
                system.<codeblock>$ gppkg -r plcontainer-1.0.0</codeblock></li>
              <li>Run the package installation command. This example installs the PL/Container 2.1.0
                language extension on a Linux
                system.<codeblock>gppkg -i plcontainer-2.1.0-gp6-rhel7_x86_64.gppkg</codeblock></li>
              <li>Source the file
                <codeph>$GPHOME/greenplum_path.sh</codeph>.<codeblock>source $GPHOME/greenplum_path.sh</codeblock></li>
              <li>Update the PL/Container configuration. This command restores the saved
                configuration.<codeblock>plcontainer runtime-restore -f plcontainer10-backup.xml</codeblock></li>
              <li>Restart Greenplum Database.<codeblock>gpstop -ra</codeblock></li>
              <li>Register the new PL/Container extension as an extension for each database that
                uses PL/Container UDFs. This <codeph>psql</codeph> command runs a <codeph>CREATE
                  EXTENSION</codeph> command to register PL/Container in the database
                  <codeph>mytest</codeph>.
                  <codeblock>psql -d mytest -c 'CREATE EXTENSION plcontainer;'</codeblock><p>The
                  command registers PL/Container as an extension and creates PL/Container-specific
                  functions and views.</p></li>
            </ol></p>
          <p>After upgrading PL/Container for a database, re-install any user-created PL/Container
            UDFs that are required.</p>
        </section>
        <section>
          <title>Upgrade from PL/Container 1.1</title>
          <p>To upgrade from PL/Container 1.1 or higher, you save the current configuration, upgrade
            PL/Container, and then restore the configuration after upgrade. There is no need to
            update the Docker images when you upgrade PL/Container.</p>
          <note>Before you perform this upgrade procedure, ensure that you have migrated your
            PL/Container 1.1 package from your previous Greenplum Database installation to your new
            Greenplum Database installation. Refer to the <xref
              href="../utility_guide/ref/gppkg.xml#topic1" format="dita" scope="peer">gppkg</xref>
            command for package installation and migration information.</note>
          <p>Perform the following procedure to upgrade from PL/Container 1.1 to PL/Container
            version 1.2 or later.<ol id="ol_l1j_bml_vkb">
              <li>Save the PL/Container configuration. For example, to save the configuration to a
                file named <codeph>plcontainer110-backup.xml</codeph> in the local
                directory:<codeblock>$ plcontainer runtime-backup -f plcontainer110-backup.xml</codeblock></li>
              <li>Use the Greenplum Database <codeph>gppkg</codeph> utility with the
                  <codeph>-u</codeph> option to update the PL/Container language extension. For
                example, the following command updates the PL/Container language extension to
                version 2.1.0 on a Linux
                system:<codeblock>$ gppkg -u plcontainer-2.1.0-gp6-rhel7_x86_64.gppkg</codeblock></li>
              <li>Source the Greenplum Database environment file
                  <codeph>$GPHOME/greenplum_path.sh</codeph>.<codeblock>$ source $GPHOME/greenplum_path.sh</codeblock></li>
              <li>Restore the PL/Container configuration. For example, this command restores the
                PL/Container configuration that you saved in a previous step:
                <codeblock>$ plcontainer runtime-restore -f plcontainer110-backup.xml</codeblock></li>
              <li>Restart Greenplum Database.<codeblock>$ gpstop -ra</codeblock></li>
              <li>You do not need to re-register the PL/Container extension in the databases in
                which you previously created the extension. You must register the PL/Container
                extension in each new database that will run PL/Container UDFs. For example, the
                following command registers PL/Container in a database named
                <codeph>mytest</codeph>:
                  <codeblock>$ psql -d mytest -c 'CREATE EXTENSION plcontainer;'</codeblock><p>The
                  command also creates PL/Container-specific functions and views.</p></li>
            </ol></p>
          <note>PL/Container 1.2 and later utilizes the new resource group capabilities introduced
            in Greenplum Database 5.8.0. If you downgrade to a Greenplum Database system that uses
            PL/Container 1.1. or earlier, you must use <codeph>plcontainer runtime-edit</codeph> to
            remove any <codeph>resource_group_id</codeph> settings from your PL/Container runtime
            configuration.</note>
        </section>
      </body>
    </topic>
    
    
    <topic id="topic_i2t_v2n_sbb" otherprops="oss-only">
      <title>Building and Installing the PL/Container Language Extension</title>
      <!--oss only content-->
      <body>
        <p>The PL/Container language extension is available as an open source module. For
          information about the building and installing the module as part of Greenplum Database,
          see the <codeph>README</codeph> file in the GitHub repository at <xref
            href="https://github.com/greenplum-db/plcontainer" format="html" scope="external"
            >https://github.com/greenplum-db/plcontainer</xref>.</p>
      </body>
    </topic>

  
  <topic id="topic6" xml:lang="en">
    <title id="pz213704">Uninstalling PL/Container</title>
    <body>
      <p>To uninstall PL/Container, remove Docker containers and images, and then remove the
        PL/Container support from Greenplum Database.</p>
      <p>When you remove support for PL/Container, the <codeph>plcontainer</codeph> user-defined
        functions that you created in the database will no longer work. </p>
    </body>
    <topic id="topic_rnb_4s5_lw">
      <title>Uninstall Docker Containers and Images</title>
      <body>
        <p>On the Greenplum Database hosts, uninstall the Docker containers and images that are no
          longer required. </p>
        <p>The <codeph>plcontainer image-list</codeph> command lists the Docker images that are
          installed on the local Greenplum Database host. </p>
        <p>The <codeph>plcontainer image-delete</codeph> command deletes a specified Docker image
          from all Greenplum Database hosts. </p>
        <p>Some Docker containers might exist on a host if the containers were not managed by
          PL/Container. You might need to remove the containers with Docker commands. These
            <codeph>docker</codeph> commands manage Docker containers and images on a local host.<ul
            id="ul_emd_ts5_lw">
            <li>The command <codeph>docker ps -a</codeph> lists all containers on a host. The
              command <codeph>docker stop</codeph> stops a container.</li>
            <li>The command <codeph>docker images</codeph> lists the images on a host.</li>
            <li>The command <codeph>docker rmi</codeph> removes images.</li>
            <li>The command <codeph>docker rm</codeph> removes containers. </li>
          </ul></p>
      </body>
    </topic>
    <topic xml:lang="en" id="topic_qnb_3cj_kw">
      <title>Remove PL/Container Support for a Database</title>
      <body>
        <p>For a database that no long requires PL/Container, remove support for PL/Container.</p>
        <section>
          <title>PL/Container 1.1 and Later</title>
          <p>For PL/Container 1.1 and later, drop the extension from the database. This
              <codeph>psql</codeph> command runs a <codeph>DROP EXTENION</codeph> command to remove
            PL/Container in the database
            <codeph>mytest</codeph>.<codeblock>psql -d mytest -c 'DROP EXTENSION plcontainer CASCADE;'</codeblock></p>
          <p>The command drops the <codeph>plcontainer</codeph> extension. The
              <codeph>CASCADE</codeph> keyword drops PL/Container-specific functions and views from
            the database.</p>
        </section>
        <section>
          <title>PL/Container 1.0</title>
          <p>Run the <codeph>plcontainer_uninstall.sql</codeph> script as the
              <codeph>gpadmin</codeph> user. For example, this command removes the
              <codeph>plcontainer</codeph> language in the <codeph>mytest</codeph> database. </p>
          <codeblock>psql -d mytest -f $GPHOME/share/postgresql/plcontainer/plcontainer_uninstall.sql</codeblock>
          <p>The script drops the <codeph>plcontainer</codeph> language with
              <codeph>CASCADE</codeph> to drop PL/Container-specific functions and views from the
            database.</p>
        </section>
      </body>
    </topic>
    <topic xml:lang="en" id="topic_dty_fcj_kw" otherprops="pivotal">
      <title>Uninstalling PL/Container Language Extension</title>
      <body>
        <p>If no databases have <codeph>plcontainer</codeph> as a registered language, uninstall the
          Greenplum Database PL/Container language extension with the <codeph>gppkg</codeph>
          utility. </p>
        <ol id="ol_ety_fcj_kw">
          <li>Use the Greenplum Database <codeph>gppkg</codeph> utility with the <codeph>-r</codeph>
            option to uninstall the PL/Container language extension. This example uninstalls the
            PL/Container language extension on a Linux
              system:<codeblock>$ gppkg -r plcontainer-2.1.0</codeblock><p>You can run the
                <codeph>gppkg</codeph> utility with the options <codeph>-q --all</codeph> to list
              the installed extensions and their versions.</p></li>
          <li>Reload
            <codeph>greenplum_path.sh</codeph>.<codeblock>$ source $GPHOME/greenplum_path.sh</codeblock></li>
          <li>Restart the database.<codeblock>$ gpstop -ra</codeblock></li>
        </ol>
      </body>
    </topic>
  </topic>
  
  
  
  <topic id="topic_sk1_gdq_dw">
    <title>Configure PL/Container</title>
    <body>
      <p>The Greenplum Database utility <codeph>plcontainer</codeph> manages the PL/Container
        configuration files in a Greenplum Database system. The utility ensures that the
        configuration files are consistent across the Greenplum Database master and segment
        instances.</p>
      <note type="warning"> Modifying the configuration files on the segment instances without using
        the utility might create different, incompatible configurations on different Greenplum
        Database segments that could cause unexpected behavior. </note>
      <p>Configuration changes that are made with the utility are applied to the XML files on all
        Greenplum Database segments. However, PL/Container configurations of currently running
        sessions use the configuration that existed during session start up. To update the
        PL/Container configuration in a running session, execute this command in the session.</p>
      <codeblock>SELECT * FROM plcontainer_refresh_config;</codeblock>
      <p>Running the command executes a PL/Container function that updates the session configuration
        on the master and segment instances.</p>
 
    
    <section id="topic_ojn_r2s_dw">
      <title>PL/Container Configuration File</title>
      
        <p>PL/Container maintains a configuration file
            <codeph>plcontainer_configuration.xml</codeph> in the data directory of all Greenplum
          Database segments. In this XML file, the root element <codeph>configuration</codeph> contains one or more
            <codeph>runtime</codeph> elements. You specify the <codeph>id</codeph> of the
            <codeph>runtime</codeph> element in the <codeph># container:</codeph> line of a
          PL/Container function definition. </p>
       <p>This query lists the Greenplum Database system data
        directories:
         <codeblock>SELECT hostname, datadir FROM gp_segment_configuration;</codeblock>
         A sample PL/Container configuration file is in
          <codeph>$GPHOME/share/postgresql/plcontainer</codeph>. </p>
        <p>In an XML file, names, such as element and attribute names, and values are case
          sensitive.</p>
        <p>This is an example
          file.<codeblock>&lt;?xml version="1.0" ?>
&lt;configuration>
    &lt;runtime>
        &lt;id>plc_python_example1&lt;/id>
        &lt;image>pivotaldata/plcontainer_python_with_clients:0.1&lt;/image>
        &lt;command>./pyclient&lt;/command>
    &lt;/runtime>
    &lt;runtime>
        &lt;id>plc_python_example2&lt;/id>
        &lt;image>pivotaldata/plcontainer_python_without_clients:0.1&lt;/image>
        &lt;command>/clientdir/pyclient.sh&lt;/command>
        &lt;shared_directory access="ro" container="/clientdir" host="/usr/local/greenplum-db/bin/plcontainer_clients"/>
        &lt;setting memory_mb="512"/>
        &lt;setting use_container_logging="yes"/>
        &lt;setting cpu_share="1024"/>
        &lt;setting resource_group_id="16391"/>
    &lt;/runtime>
    &lt;runtime>
        &lt;id>plc_r_example&lt;/id>
        &lt;image>pivotaldata/plcontainer_r_without_clients:0.2&lt;/image>
        &lt;command>/clientdir/rclient.sh&lt;/command>
        &lt;shared_directory access="ro" container="/clientdir" host="/usr/local/greenplum-db/bin/plcontainer_clients"/>
        &lt;setting use_container_logging="yes"/>
        &lt;setting roles="gpadmin,user1"/>
    &lt;/runtime>
    &lt;runtime>
&lt;/configuration></codeblock></p>
        <p>These are the XML elements and attributes in a PL/Container configuration file.</p>
        <parml>
          <plentry>
            <pt>configuration</pt>
            <pd>Root element for the XML file.</pd>
          </plentry>
          <plentry>
            <pt>runtime</pt>
            <pd>One element for each specific container available in the system. These are child
              elements of the <codeph>configuration</codeph> element.</pd>
            <pd>
              <parml>
                <plentry>
                  <pt>id</pt>
                  <pd>Required. The value is used to reference a Docker container from a
                    PL/Container user-defined function. The <codeph>id</codeph> value must be unique
                    in the configuration. The <codeph>id</codeph> must start with a character or
                    digit (a-z, A-Z, or 0-9) and can contain characters, digits, or the characters
                      <codeph>_</codeph> (underscore), <codeph>.</codeph> (period), or
                      <codeph>-</codeph> (dash). Maximum length is 63 Bytes.<p>The
                        <codeph>id</codeph> specifies which Docker image to use when PL/Container
                      creates a Docker container to execute a user-defined function.</p></pd>
                </plentry>
                <plentry>
                  <pt>image</pt>
                  <pd>
                    <p>Required. The value is the full Docker image name, including image tag. The
                      same way you specify them for starting this container in Docker. Configuration
                      allows to have many container objects referencing the same image name, this
                      way in Docker they would be represented by identical containers. </p>
                    <p>For example, you might have two <codeph>runtime</codeph> elements, with
                      different <codeph>id</codeph> elements, <codeph>plc_python_128</codeph> and
                        <codeph>plc_python_256</codeph>, both referencing the Docker image
                        <codeph>pivotaldata/plcontainer_python:1.0.0</codeph>. The first
                        <codeph>runtime</codeph> specifies a 128MB RAM limit and the second one
                      specifies a 256MB limit that is specified by the <codeph>memory_mb</codeph>
                      attribute of a <codeph>setting</codeph> element.</p>
                  </pd>
                </plentry>
                <plentry>
                  <pt>command</pt>
                  <pd>Required. The value is the command to be run inside of container to start the
                    client process inside in the container. When creating a <codeph>runtime</codeph>
                    element, the <codeph>plcontainer</codeph> utility adds a
                      <codeph>command</codeph> element based on the language (the
                      <codeph>-l</codeph> option).</pd>
                  <pd><codeph>command</codeph> element for the Python 2
                    language.<codeblock>&lt;command>/clientdir/pyclient.sh&lt;/command></codeblock></pd>
                  <pd><codeph>command</codeph> element for the Python 3
                    language.<codeblock>&lt;command>/clientdir/pyclient3.sh&lt;/command></codeblock></pd>
                  <pd><codeph>command</codeph> element for the R
                    language.<codeblock>&lt;command>/clientdir/rclient.sh&lt;/command></codeblock></pd>
                  <pd>You should modify the value only if you build a custom container and want to
                    implement some additional initialization logic before the container starts.
                    <note>This element cannot be set with the <codeph>plcontainer</codeph> utility.
                      You can update the configuration file with the <codeph>plcontainer
                        runtime-edit</codeph> command.</note></pd>
                </plentry>
                <plentry>
                  <pt>shared_directory</pt>
                  <pd>Optional. This element specifies a shared Docker shared volume for a container
                    with access information. Multiple <codeph>shared_directory</codeph> elements are
                    allowed. Each <codeph>shared_directory</codeph> element specifies a single
                    shared volume. XML attributes for the <codeph>shared_directory</codeph>
                      element:<ul id="ul_x4d_lcs_dw">
                      <li><codeph>host</codeph> - a directory location on the host system.</li>
                      <li><codeph>container</codeph> - a directory location inside of
                        container.</li>
                      <li><codeph>access</codeph> - access level to the host directory, which can be
                        either <codeph>ro</codeph> (read-only) or <codeph>rw</codeph> (read-write).
                      </li>
                    </ul></pd>
                  <pd>When creating a <codeph>runtime</codeph> element, the
                      <codeph>plcontainer</codeph> utility adds a <codeph>shared_directory</codeph>
                    element.<codeblock>&lt;shared_directory access="ro" container="/clientdir" host="/usr/local/greenplum-db/bin/plcontainer_clients"/></codeblock></pd>
                  <pd>For each <codeph>runtime</codeph> element, the <codeph>container</codeph>
                    attribute of the <codeph>shared_directory</codeph> elements must be unique. For
                    example, a <codeph>runtime</codeph> element cannot have two
                      <codeph>shared_directory</codeph> elements with attribute
                      <codeph>container="/clientdir"</codeph>.
                    <note type="warning">Allowing read-write access to a host directory requires
                      special consideration.<ul id="ul_vzb_dvk_kcb">
                        <li>When specifying read-write access to host directory, ensure that the
                          specified host directory has the correct permissions. </li>
                        <li>When running PL/Container user-defined functions, multiple concurrent
                          Docker containers that are running on a host could change data in the host
                          directory. Ensure that the functions support multiple concurrent access to
                          the data in the host directory.</li>
                      </ul></note></pd>
                </plentry>
                <plentry id="plc_settings">
                  <pt>settings</pt>
                  <pd>Optional. This element specifies Docker container configuration information.
                    Each <codeph>setting</codeph> element contains one attribute. The element
                    attribute specifies logging, memory, or networking information. For example,
                    this element enables
                    logging.<codeblock>&lt;setting use_container_logging="yes"/></codeblock></pd>
                  <pd>These are the valid attributes.<parml>
                      <plentry>
                        <pt>cpu_share</pt>
                        <pd>Optional. Specify the CPU usage for each PL/Container container in the
                          runtime. The value of the element is a positive integer. The default value
                          is 1024. The value is a relative weighting of CPU usage compared to other
                          containers. </pd>
                        <pd>For example, a container with a <codeph>cpu_share</codeph> of 2048 is
                          allocated double the CPU slice time compared with container with the
                          default value of 1024.</pd>
                      </plentry>
                      <plentry>
                        <pt>memory_mb="<varname>size</varname>"</pt>
                        <pd>Optional. The value specifies the amount of memory, in MB, that each
                          container is allowed to use. Each container starts with this amount of RAM
                          and twice the amount of swap space. The container memory consumption is
                          limited by the host system <codeph>cgroups</codeph> configuration, which
                          means in case of memory overcommit, the container is terminated by the
                          system.</pd>
                      </plentry>
                      <plentry>
                        <pt>resource_group_id="<varname>rg_groupid</varname>"</pt>
                        <pd>Optional. The value specifies the <codeph>groupid</codeph> of the
                          resource group to assign to the PL/Container runtime. The resource group
                          limits the total CPU and memory resource usage for all running containers
                          that share this runtime configuration. You must specify the
                            <codeph>groupid</codeph> of the resource group. If you do not assign a
                          resource group to a PL/Container runtime configuration, its container
                          instances are limited only by system resources. For information about
                          managing PL/Container resources, see <xref href="#topic_resmgmt"
                            format="dita">About PL/Container Resource Management</xref>.</pd>
                      </plentry>
                      <plentry>
                        <pt>roles="<varname>list_of_roles</varname>"</pt>
                        <pd>Optional. The value is a Greenplum Database role name or a
                          comma-separated list of roles. PL/Container runs a container that uses the
                          PL/Container runtime configuration only for the listed roles. If the
                          attribute is not specified, any Greenplum Database role can run an
                          instance of this container runtime configuration. For example, you create
                          a UDF that specifies the <codeph>plcontainer</codeph> language and
                          identifies a <codeph># container:</codeph> runtime configuration that has
                          the <codeph>roles</codeph> attribute set. When a role (user) runs the UDF,
                          PL/Container checks the list of roles and runs the container only if the
                          role is on the list.</pd>
                      </plentry>
                      <plentry>
                        <pt> use_container_logging="{yes | no}"</pt>
                        <pd>Optional. Enables or disables Docker logging for the container. The
                          attribute value <codeph>yes</codeph> enables logging. The attribute value
                            <codeph>no</codeph> disables logging (the default). </pd>
                        <pd>The Greenplum Database server configuration parameter <codeph><xref
                              href="../ref_guide/config_params/guc-list.xml#log_min_messages"
                              scope="peer">log_min_messages</xref></codeph> controls the
                          PL/Container log level. The default log level is <codeph>warning</codeph>.
                          For information about PL/Container log information, see <xref
                            href="#plc_notes" format="dita">Notes</xref>.</pd>
                        <pd>
                          <p>By default, the PL/Container log information is sent to a system
                            service. On Red Hat 7 or CentOS 7 systems, the log information is sent
                            to the <codeph>journald</codeph> service. On Red Hat 6 or CentOS 6
                            systems, the log is sent to the <codeph>syslogd</codeph> service. </p>
                        </pd>
                      </plentry>
                    </parml></pd>
                </plentry>
              </parml>
            </pd>
          </plentry>
        </parml>
      
    </section>
    
    <section id="topic_v3s_qv3_kw">
      <title>Update the PL/Container Configuration</title>
      
        <p>You can add a <codeph>runtime</codeph> element to the PL/Container configuration file
          with the <codeph>plcontainer runtime-add</codeph> command. The command options specify
          information such as the runtime ID, Docker image, and language. You can use the
            <codeph>plcontainer runtime-replace</codeph> command to update an existing
            <codeph>runtime</codeph> element. The utility updates the configuration file on the
          master and all segment instances.</p>
        <p>The PL/Container configuration file can contain multiple <codeph>runtime</codeph>
          elements that reference the same Docker image specified by the XML element
            <codeph>image</codeph>. In the example configuration file, the <codeph>runtime</codeph>
          elements contain <codeph>id</codeph> elements named <codeph>plc_python_128</codeph> and
            <codeph>plc_python_256</codeph>, both referencing the Docker container
            <codeph>pivotaldata/plcontainer_python:1.0.0</codeph>. The first
            <codeph>runtime</codeph> element is defined with a 128MB RAM limit and the second one
          with a 256MB RAM limit.</p>
        <codeblock>&lt;configuration>
  &lt;runtime>
    &lt;id>plc_python_128&lt;/id>
    &lt;image>pivotaldata/plcontainer_python:1.0.0&lt;/image>
    &lt;command>./client&lt;/command>
    &lt;shared_directory access="ro" container="/clientdir" host="/usr/local/gpdb/bin/plcontainer_clients"/>
    &lt;setting memory_mb="128"/>
  &lt;/runtime>
  &lt;runtime>
    &lt;id>plc_python_256&lt;/id>
    &lt;image>pivotaldata/plcontainer_python:1.0.0&lt;/image>
    &lt;command>./client&lt;/command>
    &lt;shared_directory access="ro" container="/clientdir" host="/usr/local/gpdb/bin/plcontainer_clients"/>
    &lt;setting memory_mb="256"/>
    &lt;setting resource_group_id="16391"/>
  &lt;/runtime>
&lt;configuration></codeblock>
      
    </section>
    </body>
  </topic>  

    
    <topic id="plc_notes">
      <title>Notes</title>
      <body>
        <ul id="ul_j4g_vgs_wbb">
          <li>PL/Container does not support the Greenplum Database domain object.</li>
          
          <li>When Greenplum Database executes a PL/Container UDF, Query Executer (QE) processes
            start Docker containers and reuse them as needed. After a certain amount of idle time, a
            QE process quits and destroys its Docker containers. You can control the amount of idle
            time with the Greenplum Database server configuration parameter <codeph><xref
                href="../ref_guide/config_params/guc-list.xml#gp_vmem_idle_resource_timeout"
                scope="peer">gp_vmem_idle_resource_timeout</xref></codeph>. Controlling the idle
            time might help with Docker container reuse and avoid the overhead of creating and
            starting a Docker container. <note type="warning">Changing
                <codeph>gp_vmem_idle_resource_timeout</codeph> value, might affect performance due
              to resource issues. The parameter also controls the freeing of Greenplum Database
              resources other than Docker containers.</note></li>
          <li>If a PL/Container Docker container exceeds the maximum allowed memory, it is
            terminated and an out of memory warning is displayed. On Red Hat 6 or CentOS 6 systems
            that are configured with Docker version 1.7.1, the out of memory warning is also
            displayed if the PL/Container Docker container main program (PID 1) is terminated.</li>
          <li>In some cases, when PL/Container is running in a high concurrency environment, the
            Docker daemon hangs with log entries that indicate a memory shortage. This can happen
            even when the system seems to have adequate free memory.<p>The issue seems to be
              triggered by a combination of two factors, the aggressive virtual memory requirement
              of the Go language (<codeph>golang</codeph>) runtime that is used by PL/Container, and
              the Greenplum Database Linux server kernel parameter setting for
                <codeph>overcommit_memory</codeph>. The parameter is set to 2 which does not allow
              memory overcommit. </p><p>A workaround that might help is to increase the amount of
              swap space and increase the Linux server kernel parameter
                <codeph>overcommit_ratio</codeph>. If the issue still occurs after the changes,
              there might be memory shortage. You should check free memory on the system and add
              more RAM if needed. You can also decrease the cluster load.</p></li>
          <li>PL/Container does not limit the Docker base device size, the size of the Docker
            container. In some cases, the Docker daemon controls the base device size. For example,
            if the Docker storage driver is devicemapper, the Docker daemon
              <codeph>--storage-opt</codeph> option flag <codeph>dm.basesize</codeph> controls the
            base device size. The default base device size for devicemapper is 10GB. The Docker
            command <codeph>docker info</codeph> displays Docker system information including the
            storage driver. The base device size is displayed in Docker 1.12 and later. For
            information about Docker storage drivers, see the Docker information <xref
              href="https://docs.docker.com/engine/reference/commandline/dockerd/#daemon-storage-driver"
              format="html" scope="external">Daemon storage-driver</xref>. <p>When setting the
              Docker base device size, the size must be set on all Greenplum Database
            hosts.</p></li>
          <li>When PL/Container logging is enabled, you can set the log level with the Greenplum
            Database server configuration parameter <codeph><xref
                href="../ref_guide/config_params/guc-list.xml#log_min_messages" scope="peer"
                >log_min_messages</xref></codeph>. The default log level is
            <codeph>warning</codeph>. The parameter controls the PL/Container log level and also
            controls the Greenplum Database log level.<ul id="ul_knd_jhl_mcb">
              <li>PL/Container logging is enabled or disabled for each runtime ID with the
                  <codeph>setting</codeph> attribute <codeph>use_container_logging</codeph>. The
                default is no logging. </li>
              <li>The PL/Container log information is the information from the UDF that is run in
                the Docker container. By default, the PL/Container log information is sent to a
                system service. On Red Hat 7 or CentOS 7 systems, the log information is sent to the
                  <codeph>journald</codeph> service. On Red Hat 6 or CentOS 6 systems, the log
                information is sent to the <codeph>syslogd</codeph> service. The PL/Container log
                information is sent to the log file of the host were the Docker container runs. </li>
              <li>The Greenplum Database log information is sent to log file on the Greenplum
                Database master.</li>
            </ul><p>When testing or troubleshooting a PL/Container UDF, you can change the Greenplum
              Database log level with the <codeph>SET</codeph> command. You can set the parameter in
              the session before you run your PL/Container UDF. This example sets the log level to
                <codeph>debug1</codeph>.</p><codeblock>SET log_min_messages='debug1' ;</codeblock>
            <note>The parameter <codeph>log_min_messages</codeph> controls both the Greenplum
              Database and PL/Container logging, increasing the log level might affect Greenplum
              Database performance even if a PL/Container UDF is not running.</note></li>
        </ul>
      </body>
    </topic>
  </topic>
    
    
  <topic xml:lang="en" id="topic_kds_plk_rbb">
    <title>References</title>
    <body>
      <p>Docker home page <xref href="https://www.docker.com/" format="html" scope="external"
          >https://www.docker.com/</xref></p>
      <p>Docker command line interface <xref
          href="https://docs.docker.com/engine/reference/commandline/cli/" format="html"
          scope="external">https://docs.docker.com/engine/reference/commandline/cli/</xref></p>
      <p>Dockerfile reference <xref href="https://docs.docker.com/engine/reference/builder/"
          format="html" scope="external"
        >https://docs.docker.com/engine/reference/builder/</xref></p>
      <p>Installing Docker on Linux systems <xref
          href="https://docs.docker.com/engine/installation/linux/centos/" format="html"
          scope="external">https://docs.docker.com/engine/installation/linux/centos/</xref></p>
      <p>Control and configure Docker with systemd <xref
          href="https://docs.docker.com/engine/admin/systemd/" format="html" scope="external"
          >https://docs.docker.com/engine/admin/systemd/</xref></p>
      <p>Changes to Python <xref href="https://docs.python.org/3/whatsnew/index.html" format="html"
          scope="external">What’s New in Python</xref></p>
      <p>Porting from Python 2 to 3 <xref href="https://docs.python.org/3/howto/pyporting.html"
          format="html" scope="external">Porting Python 2 Code to Python 3</xref></p>
    </body>
  </topic>
</topic>