<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic
  PUBLIC "-//OASIS//DTD DITA Composite//EN" "ditabase.dtd">
<topic id="using_plcontainer">
    <title>Using PL/Container</title>
    <body>
        <p>This topic discusses details on the PL/Container configuration file, resource management,
            and using PL/Container Python and R functions. </p>
    </body>
    <topic id="topic_sk1_gdq_dw">
        <title>PL/Container Conguration file</title>
        <body>
            <p>The Greenplum Database utility <codeph>plcontainer</codeph> manages the PL/Container
                configuration files in a Greenplum Database system. The utility ensures that the
                configuration files are consistent across the Greenplum Database master and segment
                instances.</p>
            <note type="warning"> Modifying the configuration files on the segment instances without
                using the utility might create different, incompatible configurations on different
                Greenplum Database segments that could cause unexpected behavior. </note>
            <section id="topic_ojn_r2s_dw"><title>PL/Container Configuration
                    File</title>
                
                <p>PL/Container maintains a configuration file
                        <codeph>plcontainer_configuration.xml</codeph> in the data directory of all
                    Greenplum Database segments. This query lists the Greenplum Database system data
                    directories:
                </p><codeblock>SELECT hostname, datadir FROM gp_segment_configuration;</codeblock> A
                sample PL/Container configuration file is in
                    <codeph>$GPHOME/share/postgresql/plcontainer</codeph>. <p>In an XML file, names,
                    such as element and attribute names, and values are case sensitive.</p>In this
                XML file, the root element <codeph>configuration</codeph> contains one or more
                    <codeph>runtime</codeph> elements. You specify the <codeph>id</codeph> of the
                    <codeph>runtime</codeph> element in the <codeph># container:</codeph> line of a
                PL/Container function definition. <p>This is an example file. Note that all XML
                    elements, names, and attributes are case
                    sensitive.<codeblock>&lt;?xml version="1.0" ?>
&lt;configuration>
    &lt;runtime>
        &lt;id>plc_python_example1&lt;/id>
        &lt;image>pivotaldata/plcontainer_python_with_clients:0.1&lt;/image>
        &lt;command>./pyclient&lt;/command>
    &lt;/runtime>
    &lt;runtime>
        &lt;id>plc_python_example2&lt;/id>
        &lt;image>pivotaldata/plcontainer_python_without_clients:0.1&lt;/image>
        &lt;command>/clientdir/pyclient.sh&lt;/command>
        &lt;shared_directory access="ro" container="/clientdir" host="/usr/local/greenplum-db/bin/plcontainer_clients"/>
        &lt;setting memory_mb="512"/>
        &lt;setting use_container_logging="yes"/>
        &lt;setting cpu_share="1024"/>
        &lt;setting resource_group_id="16391"/>
    &lt;/runtime>
    &lt;runtime>
        &lt;id>plc_r_example&lt;/id>
        &lt;image>pivotaldata/plcontainer_r_without_clients:0.2&lt;/image>
        &lt;command>/clientdir/rclient.sh&lt;/command>
        &lt;shared_directory access="ro" container="/clientdir" host="/usr/local/greenplum-db/bin/plcontainer_clients"/>
        &lt;setting use_container_logging="yes"/>
        &lt;setting roles="gpadmin,user1"/>
    &lt;/runtime>
    &lt;runtime>
&lt;/configuration></codeblock></p><p>These
                    are the XML elements and attributes in a PL/Container configuration file.</p><parml>
                    <plentry>
                        <pt>configuration</pt>
                        <pd>Root element for the XML file.</pd>
                    </plentry>
                    <plentry>
                        <pt>runtime</pt>
                        <pd>One element for each specific container available in the system. These
                            are child elements of the <codeph>configuration</codeph> element.</pd>
                        <pd>
                            <parml>
                                <plentry>
                                    <pt>id</pt>
                                    <pd>Required. The value is used to reference a Docker container
                                        from a PL/Container user-defined function. The
                                            <codeph>id</codeph> value must be unique in the
                                        configuration. The <codeph>id</codeph> must start with a
                                        character or digit (a-z, A-Z, or 0-9) and can contain
                                        characters, digits, or the characters <codeph>_</codeph>
                                        (underscore), <codeph>.</codeph> (period), or
                                            <codeph>-</codeph> (dash). Maximum length is 63
                                            Bytes.<p>The <codeph>id</codeph> specifies which Docker
                                            image to use when PL/Container creates a Docker
                                            container to execute a user-defined function.</p></pd>
                                </plentry>
                                <plentry>
                                    <pt>image</pt>
                                    <pd>
                                        <p>Required. The value is the full Docker image name,
                                            including image tag. The same way you specify them for
                                            starting this container in Docker. Configuration allows
                                            to have many container objects referencing the same
                                            image name, this way in Docker they would be represented
                                            by identical containers. </p>
                                        <p>For example, you might have two <codeph>runtime</codeph>
                                            elements, with different <codeph>id</codeph> elements,
                                                <codeph>plc_python_128</codeph> and
                                                <codeph>plc_python_256</codeph>, both referencing
                                            the Docker image
                                                <codeph>pivotaldata/plcontainer_python:1.0.0</codeph>.
                                            The first <codeph>runtime</codeph> specifies a 128MB RAM
                                            limit and the second one specifies a 256MB limit that is
                                            specified by the <codeph>memory_mb</codeph> attribute of
                                            a <codeph>setting</codeph> element.</p>
                                    </pd>
                                </plentry>
                                <plentry>
                                    <pt>command</pt>
                                    <pd>Required. The value is the command to be run inside of
                                        container to start the client process inside in the
                                        container. When creating a <codeph>runtime</codeph> element,
                                        the <codeph>plcontainer</codeph> utility adds a
                                            <codeph>command</codeph> element based on the language
                                        (the <codeph>-l</codeph> option).</pd>
                                    <pd><codeph>command</codeph> element for the Python 2
                                        language.<codeblock>&lt;command>/clientdir/pyclient.sh&lt;/command></codeblock></pd>
                                    <pd><codeph>command</codeph> element for the Python 3
                                        language.<codeblock>&lt;command>/clientdir/pyclient3.sh&lt;/command></codeblock></pd>
                                    <pd><codeph>command</codeph> element for the R
                                        language.<codeblock>&lt;command>/clientdir/rclient.sh&lt;/command></codeblock></pd>
                                    <pd>You should modify the value only if you build a custom
                                        container and want to implement some additional
                                        initialization logic before the container starts. <note>This
                                            element cannot be set with the
                                                <codeph>plcontainer</codeph> utility. You can update
                                            the configuration file with the <codeph>plcontainer
                                                runtime-edit</codeph> command.</note></pd>
                                </plentry>
                                <plentry>
                                    <pt>shared_directory</pt>
                                    <pd>Optional. This element specifies a shared Docker shared
                                        volume for a container with access information. Multiple
                                            <codeph>shared_directory</codeph> elements are allowed.
                                        Each <codeph>shared_directory</codeph> element specifies a
                                        single shared volume. XML attributes for the
                                            <codeph>shared_directory</codeph> element:<ul
                                            id="ul_x4d_lcs_dw">
                                            <li><codeph>host</codeph> - a directory location on the
                                                host system.</li>
                                            <li><codeph>container</codeph> - a directory location
                                                inside of container.</li>
                                            <li><codeph>access</codeph> - access level to the host
                                                directory, which can be either <codeph>ro</codeph>
                                                (read-only) or <codeph>rw</codeph> (read-write).
                                            </li>
                                        </ul></pd>
                                    <pd>When creating a <codeph>runtime</codeph> element, the
                                            <codeph>plcontainer</codeph> utility adds a
                                            <codeph>shared_directory</codeph>
                                        element.<codeblock>&lt;shared_directory access="ro" container="/clientdir" host="/usr/local/greenplum-db/bin/plcontainer_clients"/></codeblock></pd>
                                    <pd>For each <codeph>runtime</codeph> element, the
                                            <codeph>container</codeph> attribute of the
                                            <codeph>shared_directory</codeph> elements must be
                                        unique. For example, a <codeph>runtime</codeph> element
                                        cannot have two <codeph>shared_directory</codeph> elements
                                        with attribute <codeph>container="/clientdir"</codeph>.
                                            <note type="warning">Allowing read-write access to a
                                            host directory requires special consideration.<ul
                                                id="ul_vzb_dvk_kcb">
                                                <li>When specifying read-write access to host
                                                  directory, ensure that the specified host
                                                  directory has the correct permissions. </li>
                                                <li>When running PL/Container user-defined
                                                  functions, multiple concurrent Docker containers
                                                  that are running on a host could change data in
                                                  the host directory. Ensure that the functions
                                                  support multiple concurrent access to the data in
                                                  the host directory.</li>
                                            </ul></note></pd>
                                </plentry>
                                <plentry id="plc_settings">
                                    <pt>settings</pt>
                                    <pd>Optional. This element specifies Docker container
                                        configuration information. Each <codeph>setting</codeph>
                                        element contains one attribute. The element attribute
                                        specifies logging, memory, or networking information. For
                                        example, this element enables
                                        logging.<codeblock>&lt;setting use_container_logging="yes"/></codeblock></pd>
                                    <pd>These are the valid attributes.<parml>
                                            <plentry>
                                                <pt>cpu_share</pt>
                                                <pd>Optional. Specify the CPU usage for each
                                                  PL/Container container in the runtime. The value
                                                  of the element is a positive integer. The default
                                                  value is 1024. The value is a relative weighting
                                                  of CPU usage compared to other containers. </pd>
                                                <pd>For example, a container with a
                                                  <codeph>cpu_share</codeph> of 2048 is allocated
                                                  double the CPU slice time compared with container
                                                  with the default value of 1024.</pd>
                                            </plentry>
                                            <plentry>
                                                <pt>memory_mb="<varname>size</varname>"</pt>
                                                <pd>Optional. The value specifies the amount of
                                                  memory, in MB, that each container is allowed to
                                                  use. Each container starts with this amount of RAM
                                                  and twice the amount of swap space. The container
                                                  memory consumption is limited by the host system
                                                  <codeph>cgroups</codeph> configuration, which
                                                  means in case of memory overcommit, the container
                                                  is terminated by the system.</pd>
                                            </plentry>
                                            <plentry>
                                                <pt>resource_group_id="<varname>rg_groupid</varname>"</pt>
                                                <pd>Optional. The value specifies the
                                                  <codeph>groupid</codeph> of the resource group to
                                                  assign to the PL/Container runtime. The resource
                                                  group limits the total CPU and memory resource
                                                  usage for all running containers that share this
                                                  runtime configuration. You must specify the
                                                  <codeph>groupid</codeph> of the resource group. If
                                                  you do not assign a resource group to a
                                                  PL/Container runtime configuration, its container
                                                  instances are limited only by system resources.
                                                  For information about managing PL/Container
                                                  resources, see <xref href="#topic_resmgmt"
                                                  format="dita">About PL/Container Resource
                                                  Management</xref>.</pd>
                                            </plentry>
                                            <plentry>
                                                <pt>roles="<varname>list_of_roles</varname>"</pt>
                                                <pd>Optional. The value is a Greenplum Database role
                                                  name or a comma-separated list of roles.
                                                  PL/Container runs a container that uses the
                                                  PL/Container runtime configuration only for the
                                                  listed roles. If the attribute is not specified,
                                                  any Greenplum Database role can run an instance of
                                                  this container runtime configuration. For example,
                                                  you create a UDF that specifies the
                                                  <codeph>plcontainer</codeph> language and
                                                  identifies a <codeph># container:</codeph> runtime
                                                  configuration that has the <codeph>roles</codeph>
                                                  attribute set. When a role (user) runs the UDF,
                                                  PL/Container checks the list of roles and runs the
                                                  container only if the role is on the list.</pd>
                                            </plentry>
                                            <plentry>
                                                <pt> use_container_logging="{yes | no}"</pt>
                                                <pd>Optional. Enables or disables Docker logging for
                                                  the container. The attribute value
                                                  <codeph>yes</codeph> enables logging. The
                                                  attribute value <codeph>no</codeph> disables
                                                  logging (the default). </pd>
                                                <pd>The Greenplum Database server configuration
                                                  parameter <codeph><xref
                                                  href="../ref_guide/config_params/guc-list.xml#log_min_messages"
                                                  scope="peer">log_min_messages</xref></codeph>
                                                  controls the PL/Container log level. The default
                                                  log level is <codeph>warning</codeph>. For
                                                  information about PL/Container log information,
                                                  see <xref href="#plc_notes" format="dita"
                                                  >Notes</xref>.</pd>
                                                <pd>
                                                  <p>By default, the PL/Container log information is
                                                  sent to a system service. On Red Hat 7 or CentOS 7
                                                  systems, the log information is sent to the
                                                  <codeph>journald</codeph> service. On Red Hat 6 or
                                                  CentOS 6 systems, the log is sent to the
                                                  <codeph>syslogd</codeph> service. </p>
                                                </pd>
                                            </plentry>
                                        </parml></pd>
                                </plentry>
                            </parml>
                        </pd>
                    </plentry>
                </parml>
            </section>
            <section id="topic_v3s_qv3_kw">
                <title>Update the PL/Container Configuration</title>
                <p>You can add a <codeph>runtime</codeph> element to the PL/Container configuration
                    file with the <codeph>plcontainer runtime-add</codeph> command. The command
                    options specify information such as the runtime ID, Docker image, and language.
                    You can use the <codeph>plcontainer runtime-replace</codeph> command to update
                    an existing <codeph>runtime</codeph> element. The utility updates the
                    configuration file on the master and all segment instances.</p>
                <p>The PL/Container configuration file can contain multiple <codeph>runtime</codeph>
                    elements that reference the same Docker image specified by the XML element
                        <codeph>image</codeph>. In the example configuration file, the
                        <codeph>runtime</codeph> elements contain <codeph>id</codeph> elements named
                        <codeph>plc_python_128</codeph> and <codeph>plc_python_256</codeph>, both
                    referencing the Docker container
                        <codeph>pivotaldata/plcontainer_python:1.0.0</codeph>. The first
                        <codeph>runtime</codeph> element is defined with a 128MB RAM limit and the
                    second one with a 256MB RAM limit.</p>
                <codeblock>&lt;configuration>
  &lt;runtime>
    &lt;id>plc_python_128&lt;/id>
    &lt;image>pivotaldata/plcontainer_python:1.0.0&lt;/image>
    &lt;command>./client&lt;/command>
    &lt;shared_directory access="ro" container="/clientdir" host="/usr/local/gpdb/bin/plcontainer_clients"/>
    &lt;setting memory_mb="128"/>
  &lt;/runtime>
  &lt;runtime>
    &lt;id>plc_python_256&lt;/id>
    &lt;image>pivotaldata/plcontainer_python:1.0.0&lt;/image>
    &lt;command>./client&lt;/command>
    &lt;shared_directory access="ro" container="/clientdir" host="/usr/local/gpdb/bin/plcontainer_clients"/>
    &lt;setting memory_mb="256"/>
    &lt;setting resource_group_id="16391"/>
  &lt;/runtime>
&lt;configuration></codeblock>
                <p>Configuration changes that are made with the utility are applied to the XML files
                    on all Greenplum Database segments. However, PL/Container configurations of
                    currently running sessions use the configuration that existed during session
                    start up. To update the PL/Container configuration in a running session, execute
                    this command in the session.</p>
                <codeblock>SELECT * FROM plcontainer_refresh_config;</codeblock>
                <p>Running the command executes a PL/Container function that updates the session
                    configuration on the master and segment instances.</p>
            </section>
        </body>
    </topic>
        
    <topic id="topic_rh3_p3q_dw">
        <title>PL/Container Functions</title>
        <body>
            <p>When you enable PL/Container in a database of a Greenplum Database system, the
                language <codeph>plcontainer</codeph> is registered in the database. You can create
                and run user-defined functions in the procedural languages supported by the
                PL/Container Docker images when you specify <codeph>plcontainer</codeph> as a
                language in a UDF definition.</p>
            
            <section id="topic_c3v_clg_wkb">
                <title>Limitations</title>
                <p>Review the following limitations when using PL/Container Python and R functions: </p>
                <ul id="ul_d3v_clg_wkb">
                    <li>Greenplum Database domains are not supported.</li>
                    <li>Multi-dimensional arrays are not supported.</li>
                    <li>Python and R call stack information is not displayed when debugging a
                        UDF.</li>
                    <li>The <codeph>plpy.execute()</codeph> methods <codeph>nrows()</codeph> and
                            <codeph>status()</codeph> are not supported.</li>
                    <li>The PL/Python function <codeph>plpy.SPIError()</codeph> is not
                        supported.</li>
                    <li>Executing the SAVEPOINT command with <codeph>plpy.execute()</codeph> is not
                        supported.</li>
                    <li>The DO command is not supported.</li>
                    <li>See .OUT parameters are not supported.The Python dict type cannot be
                        returned from a PL/Python UDF. </li>
                    <li>When returning the Python dict type from a UDF, you can convert the dict
                        type to a Greenplum Database user-defined data type (UDT).</li>
                </ul>
            </section>
            
            <section>
                <title>Running PL/Container functions</title>
                <p>A UDF definition that uses PL/Container must have the these items.</p>
                <ul id="ul_z2m_1kj_kw">
                    <li>The first line of the UDF must be <codeph># container:
                            <varname>ID</varname></codeph></li>
                    <li>The <codeph>LANGUAGE</codeph> attribute must be
                        <codeph>plcontainer</codeph></li>
                </ul>
                <p>The <varname>ID</varname> is the name that PL/Container uses to identify a Docker
                    image. When Greenplum Database executes a UDF on a host, the Docker image on the
                    host is used to start a Docker container that runs the UDF. In the XML
                    configuration file <codeph>plcontainer_configuration.xml</codeph>, there is a
                        <codeph>runtime</codeph> XML element that contains a corresponding
                        <codeph>id</codeph> XML element that specifies the Docker container startup
                    information. See <xref href="#topic_sk1_gdq_dw" format="dita"/> for information
                    about how PL/Container maps the <varname>ID</varname> to a Docker image. See
                        <xref href="#topic_kwg_qfg_mjb" format="dita"/> for example UDF
                    definitions.</p>
                <p>The PL/Container configuration file is read only on the first invocation of a
                    PL/Container function in each Greenplum Database session that runs PL/Container
                    functions. You can force the configuration file to be re-read by performing a
                        <codeph>SELECT</codeph> command on the view
                        <codeph>plcontainer_refresh_config</codeph> during the session. For example,
                    this <codeph>SELECT</codeph> command forces the configuration file to be
                    read.</p>
                <codeblock>SELECT * FROM plcontainer_refresh_config;</codeblock>
                <p>Running the command executes a PL/Container function that updates the
                    configuration on the master and segment instances and returns the status of the
                    refresh.<codeblock> gp_segment_id | plcontainer_refresh_local_config
                ---------------+----------------------------------
                1 | ok
                0 | ok
                -1 | ok
                (3 rows)</codeblock></p>
                <p>Also, you can show all the configurations in the session by performing a
                        <codeph>SELECT</codeph> command on the view
                        <codeph>plcontainer_show_config</codeph>. For example, this
                        <codeph>SELECT</codeph> command returns the PL/Container configurations. </p>
                <codeblock>SELECT * FROM plcontainer_show_config;</codeblock>
                <p>Running the command executes a PL/Container function that displays configuration
                    information from the master and segment instances. This is an example of the
                    start and end of the view
                    output.<codeblock>INFO:  plcontainer: Container 'plc_py_test' configuration
                INFO:  plcontainer:     image = 'pivotaldata/plcontainer_python_shared:devel'
                INFO:  plcontainer:     memory_mb = '1024'
                INFO:  plcontainer:     use container network = 'no'
                INFO:  plcontainer:     use container logging  = 'no'
                INFO:  plcontainer:     shared directory from host '/usr/local/greenplum-db/./bin/plcontainer_clients' to container '/clientdir'
                INFO:  plcontainer:         access = readonly
                
                ...
                
                INFO:  plcontainer: Container 'plc_r_example' configuration  (seg0 slice3 192.168.180.45:40000 pid=3304)
                INFO:  plcontainer:     image = 'pivotaldata/plcontainer_r_without_clients:0.2'  (seg0 slice3 192.168.180.45:40000 pid=3304)
                INFO:  plcontainer:     memory_mb = '1024'  (seg0 slice3 192.168.180.45:40000 pid=3304)
                INFO:  plcontainer:     use container network = 'no'  (seg0 slice3 192.168.180.45:40000 pid=3304)
                INFO:  plcontainer:     use container logging  = 'yes'  (seg0 slice3 192.168.180.45:40000 pid=3304)
                INFO:  plcontainer:     shared directory from host '/usr/local/greenplum-db/bin/plcontainer_clients' to container '/clientdir'  (seg0 slice3 192.168.180.45:40000 pid=3304)
                INFO:  plcontainer:         access = readonly  (seg0 slice3 192.168.180.45:40000 pid=3304)
                gp_segment_id | plcontainer_show_local_config
                ---------------+-------------------------------
                0 | ok
                -1 | ok
                1 | ok</codeblock></p>
                <p>The PL/Container function <codeph>plcontainer_containers_summary()</codeph>
                    displays information about the currently running Docker
                    containers.<codeblock>SELECT * FROM plcontainer_containers_summary();</codeblock></p>
                <p>If a normal (non-superuser) Greenplum Database user runs the function, the
                    function displays information only for containers created by the user. If a
                    Greenplum Database superuser runs the function, information for all containers
                    created by Greenplum Database users is displayed. This is sample output when 2
                    containers are running.</p>
                <codeblock> SEGMENT_ID |                           CONTAINER_ID                           |   UP_TIME    |  OWNER  | MEMORY_USAGE(KB)
            ------------+------------------------------------------------------------------+--------------+---------+------------------
            1          | 693a6cb691f1d2881ec0160a44dae2547a0d5b799875d4ec106c09c97da422ea | Up 8 seconds | gpadmin | 12940
            1          | bc9a0c04019c266f6d8269ffe35769d118bfb96ec634549b2b1bd2401ea20158 | Up 2 minutes | gpadmin | 13628
            (2 rows)</codeblock>
                <p>When Greenplum Database executes a PL/Container UDF, Query Executer (QE)
                    processes start Docker containers and reuse them as needed. After a certain
                    amount of idle time, a QE process quits and destroys its Docker containers. You
                    can control the amount of idle time with the Greenplum Database server
                    configuration parameter <codeph><xref
                            href="../ref_guide/config_params/guc-list.xml#gp_vmem_idle_resource_timeout"
                            scope="peer">gp_vmem_idle_resource_timeout</xref></codeph>. Controlling
                    the idle time might help with Docker container reuse and avoid the overhead of
                    creating and starting a Docker container. </p>
                <note type="warning">Changing <codeph>gp_vmem_idle_resource_timeout</codeph> value,
                    might affect performance due to resource issues. The parameter also controls the
                    freeing of Greenplum Database resources other than Docker containers.</note>
            </section>
        
        <section xml:lang="en" id="topic_kwg_qfg_mjb">
            <title id="pz215232">Examples</title>
            
                <p>The values in the <codeph># container</codeph> lines of the examples,
                        <codeph>plc_python_shared</codeph> and <codeph>plc_r_shared</codeph>, are
                    the <codeph>id</codeph> XML elements defined in the
                        <codeph>plcontainer_config.xml</codeph> file. The <codeph>id</codeph>
                    element is mapped to the <codeph>image</codeph> element that specifies the
                    Docker image to be started. If you configured PL/Container with a different ID,
                    change the value of the <codeph># container</codeph> line. For information about
                    configuring PL/Container and viewing the configuration settings, see <xref
                        href="#topic_sk1_gdq_dw" format="dita"/>.</p>
                <p>This is an example of PL/Python function that runs using the
                        <codeph>plc_python_shared</codeph> container that contains Python
                    2:<codeblock>CREATE OR REPLACE FUNCTION pylog100() RETURNS double precision AS $$
                    # container: plc_python_shared
                    import math
                    return math.log10(100)
                    $$ LANGUAGE plcontainer;</codeblock></p>
                <p>This is an example of a similar function using the <codeph>plc_r_shared</codeph>
                    container:<codeblock>CREATE OR REPLACE FUNCTION rlog100() RETURNS text AS $$
                    # container: plc_r_shared
                    return(log10(100))
                    $$ LANGUAGE plcontainer;</codeblock></p>
                <p>If the <codeph># container</codeph> line in a UDF specifies an ID that is not in
                    the PL/Container configuration file, Greenplum Database returns an error when
                    you try to execute the UDF.</p>
        </section>
        </body>
    </topic>
    
        <topic id="topic_ctk_xjg_wkb">
            <title>About PL/Container Running PL/Python </title>
            <body>
                <p>In the Python language container, the module <codeph>plpy</codeph> is
                    implemented. The module contains these methods:</p>
                <ul id="ul_dtk_xjg_wkb">
                    <li><codeph>plpy.execute(stmt)</codeph> - Executes the query string
                            <codeph>stmt</codeph> and returns query result in a list of dictionary
                        objects. To be able to access the result fields ensure your query returns
                        named fields.</li>
                    <li><codeph>plpy.prepare(stmt[, argtypes])</codeph> - Prepares the execution
                        plan for a query. It is called with a query string and a list of parameter
                        types, if you have parameter references in the query.</li>
                    <li><codeph>plpy.execute(plan[, argtypes])</codeph> - Executes a prepared
                        plan.</li>
                    <li><codeph>plpy.debug(msg)</codeph> - Sends a DEBUG2 message to the Greenplum
                        Database log.</li>
                    <li><codeph>plpy.log(msg)</codeph> - Sends a LOG message to the Greenplum
                        Database log.</li>
                    <li><codeph>plpy.info(msg)</codeph> - Sends an INFO message to the Greenplum
                        Database log.</li>
                    <li><codeph>plpy.notice(msg)</codeph> - Sends a NOTICE message to the Greenplum
                        Database log.</li>
                    <li><codeph>plpy.warning(msg)</codeph> - Sends a WARNING message to the
                        Greenplum Database log.</li>
                    <li><codeph>plpy.error(msg)</codeph> - Sends an ERROR message to the Greenplum
                        Database log. An ERROR message raised in Greenplum Database causes the query
                        execution process to stop and the transaction to rollback.</li>
                    <li><codeph>plpy.fatal(msg)</codeph> - Sends a FATAL message to the Greenplum
                        Database log. A FATAL message causes Greenplum Database session to be closed
                        and transaction to be rolled back.</li>
                    <li><codeph>plpy.subtransaction()</codeph> - Manages
                            <codeph>plpy.execute</codeph> calls in an explicit subtransaction. See
                            <xref
                            href="https://www.postgresql.org/docs/9.4/plpython-subtransaction.html"
                            format="html" scope="external">Explicit Subtransactions</xref> in the
                        PostgreSQL documentation for additional information about
                            <codeph>plpy.subtransaction()</codeph>.</li>
                </ul>
                <p>If an error of level <codeph>ERROR</codeph> or <codeph>FATAL</codeph> is raised
                    in a nested Python function call, the message includes the list of enclosing
                    functions.</p>
                <p>The Python language container supports these string quoting functions that are
                    useful when constructing ad-hoc queries. <ul id="ul_etk_xjg_wkb">
                        <li><codeph>plpy.quote_literal(string)</codeph> - Returns the string quoted
                            to be used as a string literal in an SQL statement string. Embedded
                            single-quotes and backslashes are properly doubled.
                                <codeph>quote_literal()</codeph> returns null on null input (empty
                            input). If the argument might be null, <codeph>quote_nullable()</codeph>
                            might be more appropriate.</li>
                        <li><codeph>plpy.quote_nullable(string)</codeph> - Returns the string quoted
                            to be used as a string literal in an SQL statement string. If the
                            argument is null, returns <codeph>NULL</codeph>. Embedded single-quotes
                            and backslashes are properly doubled.</li>
                        <li>
                            <codeph>plpy.quote_ident(string)</codeph> - Returns the string quoted to
                            be used as an identifier in an SQL statement string. Quotes are added
                            only if necessary (for example, if the string contains non-identifier
                            characters or would be case-folded). Embedded quotes are properly
                            doubled. </li>
                    </ul></p>
                <p>When returning text from a PL/Python function, PL/Container converts a Python
                    unicode object to text in the database encoding. If the conversion cannot be
                    performed, an error is returned.</p>
                <p>PL/Container does not support this Greenplum Database PL/Python feature:<ul
                        id="ul_ftk_xjg_wkb">
                        <li> Multi-dimensional arrays.</li>
                    </ul></p>
                <p>Also, the Python module has two global dictionary objects that retain the data
                    between function calls. They are named GD and SD. GD is used to share the data
                    between all the function running within the same container, while SD is used for
                    sharing the data between multiple calls of each separate function. Be aware that
                    accessing the data is possible only within the same session, when the container
                    process lives on a segment or master. Be aware that for idle sessions Greenplum
                    Database terminates segment processes, which means the related containers would
                    be shut down and the data from GD and SD lost.</p>
                <p>For information about PL/Python, see <xref href="pl_python.xml#topic1"/>. </p>
                <p>For information about the <codeph>plpy</codeph> methods, see <xref
                        href="https://www.postgresql.org/docs/9.4/plpython-database.html"
                        format="html" scope="external"
                        >https://www.postgresql.org/docs/9.4/plpython-database.htm</xref>. </p>
            </body>
        </topic>
        <topic id="topic_lqz_t3q_dw">
            <title>About PL/Container Running PL/R</title>
            <body>
                <p>In the R language container, the module <codeph>pg.spi</codeph> is implemented.
                    The module contains these methods:</p>
                <ul id="ul_mqz_t3q_dw">
                    <li><codeph>pg.spi.exec(stmt)</codeph> - Executes the query string
                            <codeph>stmt</codeph> and returns query result in R
                            <codeph>data.frame</codeph>. To be able to access the result fields make
                        sure your query returns named fields.</li>
                    <li><codeph>pg.spi.prepare(stmt[, argtypes])</codeph> - Prepares the execution
                        plan for a query. It is called with a query string and a list of parameter
                        types if you have parameter references in the query.</li>
                    <li><codeph>pg.spi.execp(plan[, argtypes])</codeph> - Execute a prepared
                        plan.</li>
                    <li><codeph>pg.spi.debug(msg)</codeph> - Sends a DEBUG2 message to the Greenplum
                        Database log.</li>
                    <li><codeph>pg.spi.log(msg)</codeph> - Sends a LOG message to the Greenplum
                        Database log.</li>
                    <li><codeph>pg.spi.info(msg)</codeph> - Sends an INFO message to the Greenplum
                        Database log.</li>
                    <li><codeph>pg.spi.notice(msg)</codeph> - Sends a NOTICE message to the
                        Greenplum Database log.</li>
                    <li><codeph>pg.spi.warning(msg)</codeph> - Sends a WARNING message to the
                        Greenplum Database log.</li>
                    <li><codeph>pg.spi.error(msg)</codeph> - Sends an ERROR message to the Greenplum
                        Database log. An ERROR message raised in Greenplum Database causes the query
                        execution process to stop and the transaction to rollback.</li>
                    <li><codeph>pg.spi.fatal(msg)</codeph> - Sends a FATAL message to the Greenplum
                        Database log. A FATAL message causes Greenplum Database session to be closed
                        and transaction to be rolled back.</li>
                </ul>
                <p>PL/Container does not support this PL/R feature:<ul id="ul_wjk_dgb_4cb">
                        <li> Multi-dimensional arrays.</li>
                    </ul></p>
                <p>For information about PL/R, see <xref href="pl_r.xml#topic1"/>.</p>
                <p>For information about the <codeph>pg.spi</codeph> methods, see <xref
                        href="http://www.joeconway.com/plr/doc/plr-spi-rsupport-funcs-normal.html"
                        format="html" scope="external"
                        >http://www.joeconway.com/plr/doc/plr-spi-rsupport-funcs-normal.html</xref></p>
            </body>
        </topic>
    
        <topic id="topic_resmgmt">
        <title>PL/Container Resource Management</title>
            <body>
                <section id="intro_resmgmt">
                <note>Resource Groups are available in Greenplum Database 5.8.0 and later. Only
                    PL/Container 1.2.x and later can be managed by Greenplum Database resource
                    groups. </note>
                <p>The Docker containers and the Greenplum Database servers share CPU and memory
                    resources on the same hosts. In the default case, Greenplum Database is unaware of
                    the resources consumed by running PL/Container instances. From Greenplum 5.8.0 and
                    PL/Container 1.2 and later you can use Greenplum Database resource groups to control
                    overall CPU and memory resource usage for running PL/Container instances.</p>
                <p>PL/Container manages resource usage at two levels - the container level and the
                    runtime level. You can control container-level CPU and memory resources with the
                    <codeph>memory_mb</codeph> and <codeph>cpu_share</codeph> settings that you
                    configure for the PL/Container runtime. <codeph>memory_mb</codeph> governs the
                    memory resources available to each container instance. The
                    <codeph>cpu_share</codeph> setting identifies the relative weighting of a
                    container's CPU usage compared to other containers. Refer to <xref
                        href="#topic_ojn_r2s_dw" format="dita"/> for PL/Container configuration
                    information.</p>
                <p>You cannot, by default, restrict the number of executing PL/Container container
                    instances, nor can you restrict the total amount of memory or CPU resources that
                    they consume.</p>
                </section>
                
                <section id="topic_resgroup">
                    <title>Using Resource Groups to Manage PL/Container Resources</title>
                    <p>With PL/Container 1.2.0 and later, you can use Greenplum Database resource groups to
                        manage and limit the total CPU and memory resources of containers in PL/Container
                        runtimes. For more information about enabling, configuring, and using Greenplum
                        Database resource groups, refer to <xref
                            href="../admin_guide/workload_mgmt_resgroups.xml" format="dita" scope="peer"
                            >Using Resource Groups</xref> in the <cite>Greenplum Database Administrator
                                Guide</cite>.</p>
                    <note>If you do not explicitly configure resource groups for a PL/Container runtime, its
                        container instances are limited only by system resources. The containers may consume
                        resources at the expense of the Greenplum Database server.</note>
                    <p>Resource groups for external components such as PL/Container use Linux control groups
                        (cgroups) to manage component-level use of memory and CPU resources. When you manage
                        PL/Container resources with resource groups, you configure both a memory limit and a
                        CPU limit that Greenplum Database applies to all container instances that share the
                        same PL/Container runtime configuration.</p>
                    <p>When you create a resource group to manage the resources of a PL/Container runtime,
                        you must specify <codeph>MEMORY_AUDITOR=cgroup</codeph> and
                        <codeph>CONCURRENCY=0</codeph> in addition to the required CPU and memory
                        limits. For example, the following command creates a resource group named
                        <codeph>plpy_run1_rg</codeph> for a PL/Container runtime:
                        <codeblock>CREATE RESOURCE GROUP plpy_run1_rg WITH (MEMORY_AUDITOR=cgroup, CONCURRENCY=0,
                            CPU_RATE_LIMIT=10, MEMORY_LIMIT=10);</codeblock></p>
                    <p>PL/Container does not use the <codeph>MEMORY_SHARED_QUOTA</codeph> and
                        <codeph>MEMORY_SPILL_RATIO</codeph> resource group memory limits. Refer to the
                        <codeph><xref href="../ref_guide/sql_commands/CREATE_RESOURCE_GROUP.xml"
                            format="dita" scope="peer">CREATE RESOURCE GROUP</xref></codeph> reference
                        page for detailed information about this SQL command.</p>
                    <p>You can create one or more resource groups to manage your running PL/Container
                        instances. After you create a resource group for PL/Container, you assign the
                        resource group to one or more PL/Container runtimes. You make this assignment using
                        the <codeph>groupid</codeph> of the resource group. You can determine the
                        <codeph>groupid</codeph> for a given resource group name from the
                        <codeph>gp_resgroup_config</codeph>
                        <codeph>gp_toolkit</codeph> view. For example, the following query displays the
                        <codeph>groupid</codeph> of a resource group named
                        <codeph>plpy_run1_rg</codeph>:<codeblock>SELECT groupname, groupid FROM gp_toolkit.gp_resgroup_config
                            WHERE groupname='plpy_run1_rg';
                            
                            groupname   |  groupid
                            --------------+----------
                            plpy_run1_rg |   16391
                            (1 row)</codeblock></p>
                    <p>You assign a resource group to a PL/Container runtime configuration by specifying the
                        <codeph>-s resource_group_id=<varname>rg_groupid</varname></codeph> option to
                        the <codeph>plcontainer runtime-add</codeph> (new runtime) or <codeph>plcontainer
                            runtime-replace</codeph> (existing runtime) commands. For example, to assign the
                        <codeph>plpy_run1_rg</codeph> resource group to a new PL/Container runtime named
                        <codeph>python_run1</codeph>:
                        <codeblock>plcontainer runtime-add -r python_run1 -i pivotaldata/plcontainer_python_shared:devel -l python -s resource_group_id=16391</codeblock></p>
                    <p>You can also assign a resource group to a PL/Container runtime using the
                        <codeph>plcontainer runtime-edit</codeph> command. For information about the
                        <codeph>plcontainer</codeph> command, see <xref href="#topic_rw3_52s_dw"
                            format="dita"/>.</p>
                    <p>After you assign a resource group to a PL/Container runtime, all container instances
                        that share the same runtime configuration are subject to the memory limit and the
                        CPU limit that you configured for the group. If you decrease the memory limit of a
                        PL/Container resource group, queries executing in running containers in the group
                        may fail with an out of memory error. If you drop a PL/Container resource group
                        while there are running container instances, Greenplum Database kills the running
                        containers.</p>
                </section>
                <section id="topic_resgroupcfg">
                    <title>Configuring Resource Groups for PL/Container</title>
                    <p>To use Greenplum Database resource groups to manage PL/Container resources, you must
                        explicitly configure both resource groups and PL/Container.</p>
                    <note>PL/Container 1.2 and later utilizes the new resource group capabilities introduced
                        in Greenplum Database 5.8.0. If you downgrade to a Greenplum Database system that
                        uses PL/Container 1.1. or earlier, you must use <codeph>plcontainer
                            runtime-edit</codeph> to remove any <codeph>resource_group_id</codeph> settings
                        from your PL/Container runtime configuration.</note>
                </section>
                    <section id="topic_resgroupcfg_proc">
                        <title>Procedure</title>
                        <p>Perform the following procedure to configure PL/Container to use Greenplum
                            Database resource groups for CPU and memory resource management:</p>
                        <ol>
                            <li>If you have not already configured and enabled resource groups in your
                                Greenplum Database deployment, configure cgroups and enable Greenplum
                                Database resource groups as described in <xref
                                    href="../admin_guide/workload_mgmt_resgroups.xml#topic71717999"
                                    format="dita" scope="peer">Using Resource Groups</xref> in the
                                <cite>Greenplum Database Administrator Guide</cite>. <note>If you have
                                    previously configured and enabled resource groups in your deployment,
                                    ensure that the Greenplum Database resource group
                                    <codeph>gpdb.conf</codeph> cgroups configuration file includes a
                                    <codeph>memory { }</codeph> block as described in the previous
                                    link.</note></li>
                            <li>Analyze the resource usage of your Greenplum Database deployment. Determine
                                the percentage of resource group CPU and memory resources that you want to
                                allocate to PL/Container Docker containers.</li>
                            <li>Determine how you want to distribute the total PL/Container CPU and memory
                                resources that you identified in the step above among the PL/Container
                                runtimes. Identify:<ul>
                                    <li>The number of PL/Container resource group(s) that you require.</li>
                                    <li>The percentage of memory and CPU resources to allocate to each
                                        resource group.</li>
                                    <li>The resource-group-to-PL/Container-runtime assignment(s).</li>
                                </ul></li>
                            <li>Create the PL/Container resource groups that you identified in the step
                                above. For example, suppose that you choose to allocate 25% of both memory
                                and CPU Greenplum Database resources to PL/Container. If you further split
                                these resources among 2 resource groups 60/40, the following SQL commands
                                create the resource
                                groups:<codeblock>CREATE RESOURCE GROUP plr_run1_rg WITH (MEMORY_AUDITOR=cgroup, CONCURRENCY=0,
                                    CPU_RATE_LIMIT=15, MEMORY_LIMIT=15);
                                    CREATE RESOURCE GROUP plpy_run1_rg WITH (MEMORY_AUDITOR=cgroup, CONCURRENCY=0,
                                    CPU_RATE_LIMIT=10, MEMORY_LIMIT=10);</codeblock></li>
                            <li>Find and note the <codeph>groupid</codeph> associated with each resource
                                group that you created. For
                                example:<codeblock>SELECT groupname, groupid FROM gp_toolkit.gp_resgroup_config
                                    WHERE groupname IN ('plpy_run1_rg', 'plr_run1_rg');
                                    
                                    groupname   |  groupid
                                    --------------+----------
                                    plpy_run1_rg |   16391
                                    plr_run1_rg  |   16393
                                    (1 row)</codeblock></li>
                            <li>Assign each resource group that you created to the desired PL/Container
                                runtime configuration. If you have not yet created the runtime
                                configuration, use the <codeph>plcontainer runtime-add</codeph> command. If
                                the runtime already exists, use the <codeph>plcontainer
                                    runtime-replace</codeph> or <codeph>plcontainer runtime-edit</codeph>
                                command to add the resource group assignment to the runtime configuration.
                                For example:
                                <codeblock>plcontainer runtime-add -r python_run1 -i pivotaldata/plcontainer_python_shared:devel -l python -s resource_group_id=16391
                                    plcontainer runtime-replace -r r_run1 -i pivotaldata/plcontainer_r_shared:devel -l r -s resource_group_id=16393</codeblock><p>For
                                        information about the <codeph>plcontainer</codeph> command, see <xref
                                            href="#topic_rw3_52s_dw" format="dita"/>.</p></li>
                        </ol>
                    </section>
            </body>
        </topic>
        
</topic>